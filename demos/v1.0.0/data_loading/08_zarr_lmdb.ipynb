{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 1: Header and Setup\n", "import sys\n", "sys.path.insert(0, '..')\n", "\n", "from utils.notebook_utils import display_header, display_toc, check_dependency, conclusion_box, info_box\n", "from utils.system_info import display_system_info\n", "from utils.benchmark import Benchmark, BenchmarkResult, ComparisonTable\n", "from utils.charts import setup_style, bar_comparison, throughput_comparison, memory_comparison, COLORS\n", "\n", "display_header('Chunked Storage Comparison', 'SynaDB vs Zarr vs LMDB')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 2: Table of Contents\n", "sections = [\n", "    ('Introduction', 'introduction'),\n", "    ('Setup', 'setup'),\n", "    ('Benchmark: Chunk Access', 'benchmark-chunk'),\n", "    ('Benchmark: Compression', 'benchmark-compression'),\n", "    ('Benchmark: Concurrent Access', 'benchmark-concurrent'),\n", "    ('Cloud Storage Comparison', 'cloud-storage'),\n", "    ('Append Operations Demo', 'append-operations'),\n", "    ('Results Summary', 'results'),\n", "    ('Conclusions', 'conclusions'),\n", "]\n", "display_toc(sections)"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## üìå Introduction <a id=\"introduction\"></a>\n", "\n", "This notebook compares **SynaDB** against **Zarr** and **LMDB** for chunked and memory-mapped storage:\n", "\n", "| System | Type | Key Features |\n", "|--------|------|-------------|\n", "| **SynaDB** | Embedded DB | Single-file, append-only, native compression |\n", "| **Zarr** | Array Store | Chunked N-dimensional arrays, cloud-native |\n", "| **LMDB** | Key-Value Store | Memory-mapped, high read performance |\n", "\n", "### What We'll Measure\n", "\n", "- **Chunk access performance** (partial tensor loading)\n", "- **Compression ratios** and decompression speeds\n", "- **Concurrent access** patterns\n", "- **Cloud storage** capabilities\n", "- **Append operations** for streaming data\n", "\n", "### Test Configuration\n", "\n", "- **Dataset**: Large tensor (1GB equivalent)\n", "- **Chunk size**: 1MB chunks\n", "- **Compression**: LZ4 where supported"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 4: System Info\n", "display_system_info()"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## üîß Setup <a id=\"setup\"></a>\n", "\n", "Let's set up our test environment with large tensor data."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 6: Check Dependencies and Imports\n", "import numpy as np\n", "import time\n", "import os\n", "import shutil\n", "import tempfile\n", "from pathlib import Path\n", "import matplotlib.pyplot as plt\n", "\n", "# Check for SynaDB\n", "HAS_SYNADB = check_dependency('synadb', 'pip install synadb')\n", "\n", "# Check for Zarr\n", "HAS_ZARR = check_dependency('zarr', 'pip install zarr')\n", "\n", "# Check for LMDB\n", "HAS_LMDB = check_dependency('lmdb', 'pip install lmdb')\n", "\n", "# Apply consistent styling\n", "setup_style()"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 7: Generate Large Tensor Data\n", "# Configuration - using smaller size for demo (100MB instead of 1GB)\n", "TENSOR_SHAPE = (1000, 256, 256)  # ~250MB of float32 data\n", "CHUNK_SIZE = (100, 256, 256)  # ~25MB chunks\n", "SEED = 42\n", "\n", "print(f'Generating tensor with shape {TENSOR_SHAPE}...')\n", "np.random.seed(SEED)\n", "\n", "# Generate random tensor data (float32)\n", "tensor_data = np.random.randn(*TENSOR_SHAPE).astype(np.float32)\n", "\n", "print(f'‚úì Generated tensor shape: {tensor_data.shape}')\n", "print(f'‚úì Data type: {tensor_data.dtype}')\n", "print(f'‚úì Memory usage: {tensor_data.nbytes / 1024 / 1024:.1f} MB')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 8: Create Temp Directory\n", "temp_dir = tempfile.mkdtemp(prefix='synadb_chunked_')\n", "print(f'Using temp directory: {temp_dir}')\n", "\n", "synadb_path = os.path.join(temp_dir, 'tensor_synadb.db')\n", "zarr_path = os.path.join(temp_dir, 'tensor.zarr')\n", "lmdb_path = os.path.join(temp_dir, 'tensor.lmdb')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 9: Save Data to SynaDB\n", "synadb_write_time = None\n", "\n", "if HAS_SYNADB:\n", "    from synadb import SynaDB\n", "    print('Saving data to SynaDB...')\n", "    start = time.perf_counter()\n", "    with SynaDB(synadb_path) as db:\n", "        # Store tensor in chunks\n", "        num_chunks = TENSOR_SHAPE[0] // CHUNK_SIZE[0]\n", "        for i in range(num_chunks):\n", "            chunk = tensor_data[i*CHUNK_SIZE[0]:(i+1)*CHUNK_SIZE[0]]\n", "            db.put_bytes(f'tensor/chunk_{i}', chunk.tobytes())\n", "        # Store metadata\n", "        db.put_text('tensor/shape', str(TENSOR_SHAPE))\n", "        db.put_text('tensor/dtype', str(tensor_data.dtype))\n", "        db.put_int('tensor/num_chunks', num_chunks)\n", "    synadb_write_time = time.perf_counter() - start\n", "    print(f'‚úì SynaDB: {num_chunks} chunks in {synadb_write_time:.2f}s')\n", "else:\n", "    print('‚ö†Ô∏è SynaDB not available, skipping...')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 10: Save Data to Zarr\n", "zarr_write_time = None\n", "\n", "if HAS_ZARR:\n", "    import zarr\n", "    print('Saving data to Zarr...')\n", "    start = time.perf_counter()\n", "    # Create Zarr array with chunking and compression\n", "    z = zarr.open(zarr_path, mode='w', shape=TENSOR_SHAPE, chunks=CHUNK_SIZE, dtype='float32', compressor=zarr.Blosc(cname='lz4', clevel=5))\n", "    z[:] = tensor_data\n", "    zarr_write_time = time.perf_counter() - start\n", "    print(f'‚úì Zarr: Written in {zarr_write_time:.2f}s')\n", "    print(f'  Chunks: {z.chunks}')\n", "    print(f'  Compressor: {z.compressor}')\n", "else:\n", "    print('‚ö†Ô∏è Zarr not available, skipping...')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 11: Save Data to LMDB\n", "lmdb_write_time = None\n", "\n", "if HAS_LMDB:\n", "    import lmdb\n", "    print('Saving data to LMDB...')\n", "    start = time.perf_counter()\n", "    # LMDB requires pre-allocated map size\n", "    map_size = tensor_data.nbytes * 2  # 2x for safety\n", "    env = lmdb.open(lmdb_path, map_size=map_size)\n", "    with env.begin(write=True) as txn:\n", "        num_chunks = TENSOR_SHAPE[0] // CHUNK_SIZE[0]\n", "        for i in range(num_chunks):\n", "            chunk = tensor_data[i*CHUNK_SIZE[0]:(i+1)*CHUNK_SIZE[0]]\n", "            txn.put(f'chunk_{i}'.encode(), chunk.tobytes())\n", "        txn.put(b'metadata/shape', str(TENSOR_SHAPE).encode())\n", "        txn.put(b'metadata/dtype', str(tensor_data.dtype).encode())\n", "    env.close()\n", "    lmdb_write_time = time.perf_counter() - start\n", "    print(f'‚úì LMDB: {num_chunks} chunks in {lmdb_write_time:.2f}s')\n", "else:\n", "    print('‚ö†Ô∏è LMDB not available, skipping...')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 12: Write Time Comparison\n", "write_times = {}\n", "if synadb_write_time: write_times['SynaDB'] = synadb_write_time\n", "if zarr_write_time: write_times['Zarr'] = zarr_write_time\n", "if lmdb_write_time: write_times['LMDB'] = lmdb_write_time\n", "\n", "if write_times:\n", "    fig = bar_comparison(write_times, title='Write Time (Large Tensor)', ylabel='Time (seconds)', lower_is_better=True)\n", "    plt.show()"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## üì¶ Benchmark: Chunk Access <a id=\"benchmark-chunk\"></a>\n", "\n", "Let's measure partial tensor loading performance - reading individual chunks."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 14: Chunk Access Setup\n", "NUM_CHUNK_READS = 50\n", "np.random.seed(SEED)\n", "num_chunks = TENSOR_SHAPE[0] // CHUNK_SIZE[0]\n", "random_chunk_indices = np.random.randint(0, num_chunks, size=NUM_CHUNK_READS)\n", "print(f'Testing {NUM_CHUNK_READS} random chunk reads...')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 15: SynaDB Chunk Access\n", "synadb_chunk_times = []\n", "\n", "if HAS_SYNADB:\n", "    from synadb import SynaDB\n", "    print('Benchmarking SynaDB chunk access...')\n", "    with SynaDB(synadb_path) as db:\n", "        # Warm up\n", "        for _ in range(5): _ = db.get_bytes('tensor/chunk_0')\n", "        # Benchmark\n", "        for idx in random_chunk_indices:\n", "            start = time.perf_counter()\n", "            chunk_bytes = db.get_bytes(f'tensor/chunk_{idx}')\n", "            chunk = np.frombuffer(chunk_bytes, dtype=np.float32).reshape(CHUNK_SIZE)\n", "            elapsed = (time.perf_counter() - start) * 1000\n", "            synadb_chunk_times.append(elapsed)\n", "    print(f'‚úì SynaDB: Mean chunk read {np.mean(synadb_chunk_times):.2f}ms')\n", "    print(f'  P95: {np.percentile(synadb_chunk_times, 95):.2f}ms')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 16: Zarr Chunk Access\n", "zarr_chunk_times = []\n", "\n", "if HAS_ZARR:\n", "    import zarr\n", "    print('Benchmarking Zarr chunk access...')\n", "    z = zarr.open(zarr_path, mode='r')\n", "    # Warm up\n", "    for _ in range(5): _ = z[0:CHUNK_SIZE[0]]\n", "    # Benchmark\n", "    for idx in random_chunk_indices:\n", "        start_idx = idx * CHUNK_SIZE[0]\n", "        end_idx = start_idx + CHUNK_SIZE[0]\n", "        start = time.perf_counter()\n", "        chunk = z[start_idx:end_idx]\n", "        elapsed = (time.perf_counter() - start) * 1000\n", "        zarr_chunk_times.append(elapsed)\n", "    print(f'‚úì Zarr: Mean chunk read {np.mean(zarr_chunk_times):.2f}ms')\n", "    print(f'  P95: {np.percentile(zarr_chunk_times, 95):.2f}ms')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 17: LMDB Chunk Access\n", "lmdb_chunk_times = []\n", "\n", "if HAS_LMDB:\n", "    import lmdb\n", "    print('Benchmarking LMDB chunk access...')\n", "    env = lmdb.open(lmdb_path, readonly=True)\n", "    with env.begin() as txn:\n", "        # Warm up\n", "        for _ in range(5): _ = txn.get(b'chunk_0')\n", "        # Benchmark\n", "        for idx in random_chunk_indices:\n", "            start = time.perf_counter()\n", "            chunk_bytes = txn.get(f'chunk_{idx}'.encode())\n", "            chunk = np.frombuffer(chunk_bytes, dtype=np.float32).reshape(CHUNK_SIZE)\n", "            elapsed = (time.perf_counter() - start) * 1000\n", "            lmdb_chunk_times.append(elapsed)\n", "    env.close()\n", "    print(f'‚úì LMDB: Mean chunk read {np.mean(lmdb_chunk_times):.2f}ms')\n", "    print(f'  P95: {np.percentile(lmdb_chunk_times, 95):.2f}ms')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 18: Chunk Access Results\n", "chunk_latencies = {}\n", "if synadb_chunk_times: chunk_latencies['SynaDB'] = np.mean(synadb_chunk_times)\n", "if zarr_chunk_times: chunk_latencies['Zarr'] = np.mean(zarr_chunk_times)\n", "if lmdb_chunk_times: chunk_latencies['LMDB'] = np.mean(lmdb_chunk_times)\n", "\n", "if chunk_latencies:\n", "    fig = bar_comparison(chunk_latencies, title='Chunk Read Latency', ylabel='Latency (ms)', lower_is_better=True)\n", "    plt.show()"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## üóúÔ∏è Benchmark: Compression <a id=\"benchmark-compression\"></a>\n", "\n", "Let's compare compression ratios and decompression speeds."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 20: Compression Ratio Comparison\n", "def get_size(path):\n", "    if os.path.isfile(path):\n", "        return os.path.getsize(path)\n", "    total = 0\n", "    for dirpath, _, filenames in os.walk(path):\n", "        for f in filenames:\n", "            total += os.path.getsize(os.path.join(dirpath, f))\n", "    return total\n", "\n", "original_size = tensor_data.nbytes\n", "print(f'Original tensor size: {original_size / 1024 / 1024:.1f} MB\\n')\n", "\n", "compression_ratios = {}\n", "storage_sizes = {}\n", "\n", "if os.path.exists(synadb_path):\n", "    size = get_size(synadb_path)\n", "    storage_sizes['SynaDB'] = size / (1024 * 1024)\n", "    compression_ratios['SynaDB'] = original_size / size\n", "    print(f'SynaDB: {size / 1024 / 1024:.1f} MB (ratio: {original_size / size:.2f}x)')\n", "\n", "if os.path.exists(zarr_path):\n", "    size = get_size(zarr_path)\n", "    storage_sizes['Zarr'] = size / (1024 * 1024)\n", "    compression_ratios['Zarr'] = original_size / size\n", "    print(f'Zarr: {size / 1024 / 1024:.1f} MB (ratio: {original_size / size:.2f}x)')\n", "\n", "if os.path.exists(lmdb_path):\n", "    size = get_size(lmdb_path)\n", "    storage_sizes['LMDB'] = size / (1024 * 1024)\n", "    compression_ratios['LMDB'] = original_size / size\n", "    print(f'LMDB: {size / 1024 / 1024:.1f} MB (ratio: {original_size / size:.2f}x)')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 21: Storage Size Visualization\n", "if storage_sizes:\n", "    fig = memory_comparison(storage_sizes, title='Storage Size Comparison', ylabel='Size (MB)')\n", "    plt.show()"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 22: Compression Ratio Visualization\n", "if compression_ratios:\n", "    fig = bar_comparison(compression_ratios, title='Compression Ratio (Higher is Better)', ylabel='Compression Ratio', lower_is_better=False)\n", "    plt.show()"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## üîÄ Benchmark: Concurrent Access <a id=\"benchmark-concurrent\"></a>\n", "\n", "Let's test multi-process data loading patterns."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 24: Concurrent Access Demo\n", "import threading\n", "import queue\n", "\n", "NUM_THREADS = 4\n", "READS_PER_THREAD = 10\n", "\n", "print(f'Testing concurrent access with {NUM_THREADS} threads, {READS_PER_THREAD} reads each...')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 25: SynaDB Concurrent Access\n", "synadb_concurrent_time = None\n", "\n", "if HAS_SYNADB:\n", "    from synadb import SynaDB\n", "    results_queue = queue.Queue()\n", "    \n", "    def synadb_reader(thread_id, db_path, indices):\n", "        times = []\n", "        with SynaDB(db_path) as db:\n", "            for idx in indices:\n", "                start = time.perf_counter()\n", "                _ = db.get_bytes(f'tensor/chunk_{idx % num_chunks}')\n", "                times.append(time.perf_counter() - start)\n", "        results_queue.put(times)\n", "    \n", "    print('Benchmarking SynaDB concurrent access...')\n", "    threads = []\n", "    start = time.perf_counter()\n", "    for i in range(NUM_THREADS):\n", "        indices = list(range(i * READS_PER_THREAD, (i + 1) * READS_PER_THREAD))\n", "        t = threading.Thread(target=synadb_reader, args=(i, synadb_path, indices))\n", "        threads.append(t)\n", "        t.start()\n", "    for t in threads:\n", "        t.join()\n", "    synadb_concurrent_time = time.perf_counter() - start\n", "    print(f'‚úì SynaDB: {NUM_THREADS * READS_PER_THREAD} reads in {synadb_concurrent_time:.3f}s')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 26: LMDB Concurrent Access\n", "lmdb_concurrent_time = None\n", "\n", "if HAS_LMDB:\n", "    import lmdb\n", "    \n", "    def lmdb_reader(thread_id, db_path, indices):\n", "        env = lmdb.open(db_path, readonly=True)\n", "        with env.begin() as txn:\n", "            for idx in indices:\n", "                _ = txn.get(f'chunk_{idx % num_chunks}'.encode())\n", "        env.close()\n", "    \n", "    print('Benchmarking LMDB concurrent access...')\n", "    threads = []\n", "    start = time.perf_counter()\n", "    for i in range(NUM_THREADS):\n", "        indices = list(range(i * READS_PER_THREAD, (i + 1) * READS_PER_THREAD))\n", "        t = threading.Thread(target=lmdb_reader, args=(i, lmdb_path, indices))\n", "        threads.append(t)\n", "        t.start()\n", "    for t in threads:\n", "        t.join()\n", "    lmdb_concurrent_time = time.perf_counter() - start\n", "    print(f'‚úì LMDB: {NUM_THREADS * READS_PER_THREAD} reads in {lmdb_concurrent_time:.3f}s')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 27: Concurrent Access Results\n", "concurrent_times = {}\n", "if synadb_concurrent_time: concurrent_times['SynaDB'] = synadb_concurrent_time\n", "if lmdb_concurrent_time: concurrent_times['LMDB'] = lmdb_concurrent_time\n", "\n", "if concurrent_times:\n", "    fig = bar_comparison(concurrent_times, title=f'Concurrent Access ({NUM_THREADS} threads)', ylabel='Time (seconds)', lower_is_better=True)\n", "    plt.show()"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## ‚òÅÔ∏è Cloud Storage Comparison <a id=\"cloud-storage\"></a>\n", "\n", "Zarr is designed for cloud-native storage. Let's compare the cloud storage capabilities."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 29: Cloud Storage Capabilities\n", "print('Cloud Storage Capabilities Comparison\\n')\n", "print('=' * 70)\n", "\n", "print('\\nüì¶ SynaDB:')\n", "print('  - Single file storage (easy to sync)')\n", "print('  - Can be stored on any cloud storage (S3, GCS, Azure Blob)')\n", "print('  - Requires full file download for access')\n", "print('  - Best for: Local-first workflows with cloud backup')\n", "\n", "print('\\nüì¶ Zarr:')\n", "print('  - Native cloud storage support (S3, GCS, Azure)')\n", "print('  - Chunk-level access (only download needed chunks)')\n", "print('  - Parallel chunk downloads')\n", "print('  - Best for: Large datasets accessed from cloud')\n", "\n", "print('\\nüì¶ LMDB:')\n", "print('  - Memory-mapped files (local only)')\n", "print('  - No native cloud support')\n", "print('  - Requires full database download')\n", "print('  - Best for: High-performance local access')\n", "\n", "print('\\n' + '=' * 70)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 30: Zarr Cloud Storage Demo (Simulated)\n", "if HAS_ZARR:\n", "    import zarr\n", "    print('Zarr Cloud Storage Features:\\n')\n", "    \n", "    # Show Zarr's store options\n", "    print('Available Zarr stores:')\n", "    print('  - DirectoryStore (local filesystem)')\n", "    print('  - ZipStore (single zip file)')\n", "    print('  - S3Store (Amazon S3) - requires s3fs')\n", "    print('  - GCSStore (Google Cloud Storage) - requires gcsfs')\n", "    print('  - ABSStore (Azure Blob Storage) - requires adlfs')\n", "    \n", "    # Demo with ZipStore (portable single file)\n", "    zip_path = os.path.join(temp_dir, 'tensor.zarr.zip')\n", "    print(f'\\nCreating ZipStore demo at {zip_path}...')\n", "    \n", "    store = zarr.ZipStore(zip_path, mode='w')\n", "    z = zarr.open(store, mode='w', shape=(100, 256, 256), chunks=(10, 256, 256), dtype='float32')\n", "    z[:] = tensor_data[:100]  # Store subset\n", "    store.close()\n", "    \n", "    zip_size = os.path.getsize(zip_path) / (1024 * 1024)\n", "    print(f'‚úì ZipStore created: {zip_size:.1f} MB')\n", "    print('  ‚Üí Single portable file like SynaDB!')\n", "else:\n", "    print('‚ö†Ô∏è Zarr not available, skipping cloud storage demo...')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## ‚ûï Append Operations Demo <a id=\"append-operations\"></a>\n", "\n", "Let's compare how each system handles appending new data - critical for streaming ML workloads."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 32: Append Operations Setup\n", "NUM_APPENDS = 100\n", "APPEND_CHUNK_SIZE = (10, 256, 256)\n", "print(f'Testing {NUM_APPENDS} append operations with chunk size {APPEND_CHUNK_SIZE}...')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 33: SynaDB Append Operations\n", "synadb_append_time = None\n", "\n", "if HAS_SYNADB:\n", "    from synadb import SynaDB\n", "    append_db_path = os.path.join(temp_dir, 'append_test.db')\n", "    print('Benchmarking SynaDB append operations...')\n", "    \n", "    start = time.perf_counter()\n", "    with SynaDB(append_db_path) as db:\n", "        for i in range(NUM_APPENDS):\n", "            chunk = np.random.randn(*APPEND_CHUNK_SIZE).astype(np.float32)\n", "            db.put_bytes(f'stream/chunk_{i}', chunk.tobytes())\n", "    synadb_append_time = time.perf_counter() - start\n", "    \n", "    print(f'‚úì SynaDB: {NUM_APPENDS} appends in {synadb_append_time:.3f}s')\n", "    print(f'  Throughput: {NUM_APPENDS / synadb_append_time:.0f} appends/sec')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 34: Zarr Append Operations\n", "zarr_append_time = None\n", "\n", "if HAS_ZARR:\n", "    import zarr\n", "    append_zarr_path = os.path.join(temp_dir, 'append_test.zarr')\n", "    print('Benchmarking Zarr append operations...')\n", "    \n", "    # Zarr requires pre-allocation or resize\n", "    start = time.perf_counter()\n", "    z = zarr.open(append_zarr_path, mode='w', shape=(0, 256, 256), chunks=(10, 256, 256), dtype='float32')\n", "    for i in range(NUM_APPENDS):\n", "        chunk = np.random.randn(*APPEND_CHUNK_SIZE).astype(np.float32)\n", "        z.append(chunk, axis=0)\n", "    zarr_append_time = time.perf_counter() - start\n", "    \n", "    print(f'‚úì Zarr: {NUM_APPENDS} appends in {zarr_append_time:.3f}s')\n", "    print(f'  Throughput: {NUM_APPENDS / zarr_append_time:.0f} appends/sec')\n", "    print(f'  Final shape: {z.shape}')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 35: LMDB Append Operations\n", "lmdb_append_time = None\n", "\n", "if HAS_LMDB:\n", "    import lmdb\n", "    append_lmdb_path = os.path.join(temp_dir, 'append_test.lmdb')\n", "    print('Benchmarking LMDB append operations...')\n", "    \n", "    # LMDB requires pre-allocated map size\n", "    map_size = NUM_APPENDS * np.prod(APPEND_CHUNK_SIZE) * 4 * 2  # 2x safety margin\n", "    env = lmdb.open(append_lmdb_path, map_size=map_size)\n", "    \n", "    start = time.perf_counter()\n", "    for i in range(NUM_APPENDS):\n", "        chunk = np.random.randn(*APPEND_CHUNK_SIZE).astype(np.float32)\n", "        with env.begin(write=True) as txn:\n", "            txn.put(f'chunk_{i}'.encode(), chunk.tobytes())\n", "    lmdb_append_time = time.perf_counter() - start\n", "    env.close()\n", "    \n", "    print(f'‚úì LMDB: {NUM_APPENDS} appends in {lmdb_append_time:.3f}s')\n", "    print(f'  Throughput: {NUM_APPENDS / lmdb_append_time:.0f} appends/sec')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 36: Append Operations Results\n", "append_throughput = {}\n", "if synadb_append_time: append_throughput['SynaDB'] = NUM_APPENDS / synadb_append_time\n", "if zarr_append_time: append_throughput['Zarr'] = NUM_APPENDS / zarr_append_time\n", "if lmdb_append_time: append_throughput['LMDB'] = NUM_APPENDS / lmdb_append_time\n", "\n", "if append_throughput:\n", "    fig = throughput_comparison(append_throughput, title='Append Throughput', ylabel='Appends/second')\n", "    plt.show()"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## üìä Results Summary <a id=\"results\"></a>\n", "\n", "Let's summarize all benchmark results."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 38: Results Summary Table\n", "from IPython.display import display, Markdown\n", "\n", "# Build summary table\n", "summary_md = '''\\n| Metric | SynaDB | Zarr | LMDB |\\n|--------|--------|------|------|\\n'''\n", "\n", "# Write time\n", "synadb_wt = f'{synadb_write_time:.2f}s' if synadb_write_time else 'N/A'\n", "zarr_wt = f'{zarr_write_time:.2f}s' if zarr_write_time else 'N/A'\n", "lmdb_wt = f'{lmdb_write_time:.2f}s' if lmdb_write_time else 'N/A'\n", "summary_md += f'| Write Time | {synadb_wt} | {zarr_wt} | {lmdb_wt} |\\n'\n", "\n", "# Chunk access\n", "synadb_ca = f'{np.mean(synadb_chunk_times):.2f}ms' if synadb_chunk_times else 'N/A'\n", "zarr_ca = f'{np.mean(zarr_chunk_times):.2f}ms' if zarr_chunk_times else 'N/A'\n", "lmdb_ca = f'{np.mean(lmdb_chunk_times):.2f}ms' if lmdb_chunk_times else 'N/A'\n", "summary_md += f'| Chunk Read Latency | {synadb_ca} | {zarr_ca} | {lmdb_ca} |\\n'\n", "\n", "# Compression ratio\n", "synadb_cr = f'{compression_ratios.get(\"SynaDB\", 0):.2f}x' if 'SynaDB' in compression_ratios else 'N/A'\n", "zarr_cr = f'{compression_ratios.get(\"Zarr\", 0):.2f}x' if 'Zarr' in compression_ratios else 'N/A'\n", "lmdb_cr = f'{compression_ratios.get(\"LMDB\", 0):.2f}x' if 'LMDB' in compression_ratios else 'N/A'\n", "summary_md += f'| Compression Ratio | {synadb_cr} | {zarr_cr} | {lmdb_cr} |\\n'\n", "\n", "# Concurrent access\n", "synadb_cc = f'{synadb_concurrent_time:.3f}s' if synadb_concurrent_time else 'N/A'\n", "lmdb_cc = f'{lmdb_concurrent_time:.3f}s' if lmdb_concurrent_time else 'N/A'\n", "summary_md += f'| Concurrent Access | {synadb_cc} | N/A | {lmdb_cc} |\\n'\n", "\n", "# Append throughput\n", "synadb_at = f'{NUM_APPENDS/synadb_append_time:.0f}/s' if synadb_append_time else 'N/A'\n", "zarr_at = f'{NUM_APPENDS/zarr_append_time:.0f}/s' if zarr_append_time else 'N/A'\n", "lmdb_at = f'{NUM_APPENDS/lmdb_append_time:.0f}/s' if lmdb_append_time else 'N/A'\n", "summary_md += f'| Append Throughput | {synadb_at} | {zarr_at} | {lmdb_at} |\\n'\n", "\n", "# Features\n", "summary_md += '| Cloud Native | ‚óã Sync | ‚úì Yes | ‚úó No |\\n'\n", "summary_md += '| Single File | ‚úì Yes | ‚óã Optional | ‚úó No |\\n'\n", "summary_md += '| Compression | ‚úì LZ4 | ‚úì Multiple | ‚úó No |\\n'\n", "\n", "display(Markdown(summary_md))"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 39: Final Storage Comparison\n", "final_sizes = {}\n", "if os.path.exists(synadb_path): final_sizes['SynaDB'] = get_size(synadb_path) / (1024 * 1024)\n", "if os.path.exists(zarr_path): final_sizes['Zarr'] = get_size(zarr_path) / (1024 * 1024)\n", "if os.path.exists(lmdb_path): final_sizes['LMDB'] = get_size(lmdb_path) / (1024 * 1024)\n", "\n", "if final_sizes:\n", "    fig = memory_comparison(final_sizes, title='Final Storage Size', ylabel='Size (MB)')\n", "    plt.show()\n", "    print('\\nStorage sizes:')\n", "    for name, size in final_sizes.items():\n", "        print(f'  {name}: {size:.1f} MB')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## üéØ Conclusions <a id=\"conclusions\"></a>"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 41: Conclusions\n", "conclusion_box(\n", "    title='Key Takeaways',\n", "    points=[\n", "        'Zarr excels at cloud-native storage with chunk-level access',\n", "        'LMDB provides fastest local read performance via memory mapping',\n", "        'SynaDB offers single-file simplicity with good compression',\n", "        'For streaming/append workloads, SynaDB\\'s append-only design is ideal',\n", "        'Zarr is best for large scientific datasets accessed from cloud',\n", "        'LMDB is best for high-performance local key-value access',\n", "    ],\n", "    summary='Choose based on deployment: Zarr for cloud, LMDB for local performance, SynaDB for unified ML workflows with single-file portability.'\n", ")"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 42: Cleanup\n", "import shutil\n", "try:\n", "    shutil.rmtree(temp_dir)\n", "    print(f'‚úì Cleaned up temp directory: {temp_dir}')\n", "except Exception as e:\n", "    print(f'‚ö†Ô∏è Could not clean up: {e}')"]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.9.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}