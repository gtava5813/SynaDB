{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 1: Header and Setup\n", "import sys\n", "sys.path.insert(0, '..')\n", "\n", "from utils.notebook_utils import display_header, display_toc, check_dependency, conclusion_box, info_box\n", "from utils.system_info import display_system_info\n", "from utils.benchmark import Benchmark, BenchmarkResult, ComparisonTable\n", "from utils.charts import setup_style, bar_comparison, throughput_comparison, memory_comparison, COLORS\n", "\n", "display_header('ML Data Loading Comparison', 'SynaDB vs HDF5 vs TFRecord')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 2: Table of Contents\n", "sections = [\n", "    ('Introduction', 'introduction'),\n", "    ('Setup', 'setup'),\n", "    ('Benchmark: Sequential Load', 'benchmark-sequential'),\n", "    ('Benchmark: Random Access', 'benchmark-random'),\n", "    ('Benchmark: PyTorch DataLoader', 'benchmark-dataloader'),\n", "    ('Tensor Extraction Comparison', 'tensor-extraction'),\n", "    ('Schema Flexibility Demo', 'schema-flexibility'),\n", "    ('Results Summary', 'results'),\n", "    ('Conclusions', 'conclusions'),\n", "]\n", "display_toc(sections)"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## üìå Introduction <a id=\"introduction\"></a>\n", "\n", "This notebook compares **SynaDB's data loading capabilities** against two popular ML data formats:\n", "\n", "| System | Type | Key Features |\n", "|--------|------|-------------|\n", "| **SynaDB** | Embedded DB | Single-file, schema-free, native tensor extraction |\n", "| **HDF5** | File Format | Hierarchical, chunked, widely used in scientific computing |\n", "| **TFRecord** | File Format | TensorFlow native, sequential access optimized |\n", "\n", "### What We'll Measure\n", "\n", "- **Sequential load time** (full dataset read)\n", "- **Random access latency** (single sample retrieval)\n", "- **PyTorch DataLoader throughput** (training iteration speed)\n", "- **Tensor extraction** (direct numpy array access)\n", "- **Schema flexibility** (adding new data types)\n", "\n", "### Test Configuration\n", "\n", "- **Dataset**: MNIST-like synthetic data (60,000 images)\n", "- **Image size**: 28x28 grayscale\n", "- **Labels**: 10 classes\n", "- **Batch size**: 64 for DataLoader tests"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 4: System Info\n", "display_system_info()"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## üîß Setup <a id=\"setup\"></a>\n", "\n", "Let's set up our test environment with MNIST-like synthetic data in all three formats."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 6: Check Dependencies and Imports\n", "import numpy as np\n", "import time\n", "import os\n", "import shutil\n", "import tempfile\n", "from pathlib import Path\n", "import matplotlib.pyplot as plt\n", "\n", "# Check for SynaDB\n", "HAS_SYNADB = check_dependency('synadb', 'pip install synadb')\n", "\n", "# Check for HDF5\n", "HAS_H5PY = check_dependency('h5py', 'pip install h5py')\n", "\n", "# Check for TensorFlow (for TFRecord)\n", "HAS_TF = check_dependency('tensorflow', 'pip install tensorflow')\n", "\n", "# Check for PyTorch\n", "HAS_TORCH = check_dependency('torch', 'pip install torch')\n", "\n", "# Apply consistent styling\n", "setup_style()"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 7: Generate MNIST-like Synthetic Data\n", "NUM_SAMPLES = 60_000\n", "IMAGE_HEIGHT = 28\n", "IMAGE_WIDTH = 28\n", "NUM_CLASSES = 10\n", "SEED = 42\n", "\n", "print(f'Generating {NUM_SAMPLES:,} synthetic MNIST-like samples...')\n", "np.random.seed(SEED)\n", "\n", "# Generate synthetic images (uint8, 0-255)\n", "images = np.random.randint(0, 256, size=(NUM_SAMPLES, IMAGE_HEIGHT, IMAGE_WIDTH), dtype=np.uint8)\n", "labels = np.random.randint(0, NUM_CLASSES, size=NUM_SAMPLES, dtype=np.int64)\n", "\n", "print(f'‚úì Generated {NUM_SAMPLES:,} images')\n", "print(f'‚úì Image shape: {images.shape}')\n", "print(f'‚úì Labels shape: {labels.shape}')\n", "print(f'‚úì Memory usage: {(images.nbytes + labels.nbytes) / 1024 / 1024:.1f} MB')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 8: Create Temp Directory for Data Files\n", "temp_dir = tempfile.mkdtemp(prefix='synadb_dataload_')\n", "print(f'Using temp directory: {temp_dir}')\n", "\n", "synadb_path = os.path.join(temp_dir, 'mnist_synadb.db')\n", "hdf5_path = os.path.join(temp_dir, 'mnist.h5')\n", "tfrecord_path = os.path.join(temp_dir, 'mnist.tfrecord')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 9: Save Data to SynaDB\n", "synadb_write_time = None\n", "\n", "if HAS_SYNADB:\n", "    from synadb import SynaDB\n", "    print('Saving data to SynaDB...')\n", "    start = time.perf_counter()\n", "    with SynaDB(synadb_path) as db:\n", "        for i in range(NUM_SAMPLES):\n", "            db.put_bytes(f'image/{i}', images[i].tobytes())\n", "            db.put_int(f'label/{i}', int(labels[i]))\n", "            if (i + 1) % 10000 == 0:\n", "                print(f'  Saved {i + 1:,} samples...')\n", "    synadb_write_time = time.perf_counter() - start\n", "    print(f'‚úì SynaDB: {NUM_SAMPLES:,} samples in {synadb_write_time:.2f}s')\n", "else:\n", "    print('‚ö†Ô∏è SynaDB not available, skipping...')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 10: Save Data to HDF5\n", "hdf5_write_time = None\n", "\n", "if HAS_H5PY:\n", "    import h5py\n", "    print('Saving data to HDF5...')\n", "    start = time.perf_counter()\n", "    with h5py.File(hdf5_path, 'w') as f:\n", "        f.create_dataset('images', data=images, chunks=(1000, IMAGE_HEIGHT, IMAGE_WIDTH))\n", "        f.create_dataset('labels', data=labels, chunks=(1000,))\n", "    hdf5_write_time = time.perf_counter() - start\n", "    print(f'‚úì HDF5: {NUM_SAMPLES:,} samples in {hdf5_write_time:.2f}s')\n", "else:\n", "    print('‚ö†Ô∏è h5py not available, skipping...')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 11: Save Data to TFRecord\n", "tfrecord_write_time = None\n", "\n", "if HAS_TF:\n", "    import tensorflow as tf\n", "    print('Saving data to TFRecord...')\n", "    start = time.perf_counter()\n", "    def _bytes_feature(value):\n", "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n", "    def _int64_feature(value):\n", "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n", "    with tf.io.TFRecordWriter(tfrecord_path) as writer:\n", "        for i in range(NUM_SAMPLES):\n", "            feature = {'image': _bytes_feature(images[i].tobytes()), 'label': _int64_feature(int(labels[i]))}\n", "            example = tf.train.Example(features=tf.train.Features(feature=feature))\n", "            writer.write(example.SerializeToString())\n", "            if (i + 1) % 10000 == 0:\n", "                print(f'  Saved {i + 1:,} samples...')\n", "    tfrecord_write_time = time.perf_counter() - start\n", "    print(f'‚úì TFRecord: {NUM_SAMPLES:,} samples in {tfrecord_write_time:.2f}s')\n", "else:\n", "    print('‚ö†Ô∏è TensorFlow not available, skipping...')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 12: Write Time Comparison\n", "write_times = {}\n", "if synadb_write_time: write_times['SynaDB'] = synadb_write_time\n", "if hdf5_write_time: write_times['HDF5'] = hdf5_write_time\n", "if tfrecord_write_time: write_times['TFRecord'] = tfrecord_write_time\n", "\n", "if write_times:\n", "    fig = bar_comparison(write_times, title=f'Write Time ({NUM_SAMPLES:,} samples)', ylabel='Time (seconds)', lower_is_better=True)\n", "    plt.show()\n", "else:\n", "    print('No write time results to display.')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## ‚ö° Benchmark: Sequential Load <a id=\"benchmark-sequential\"></a>\n", "\n", "Let's measure how fast each format can load the entire dataset sequentially."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 14: SynaDB Sequential Load Benchmark\n", "synadb_seq_time = None\n", "\n", "if HAS_SYNADB:\n", "    from synadb import SynaDB\n", "    print('Benchmarking SynaDB sequential load...')\n", "    # Warm up\n", "    with SynaDB(synadb_path) as db:\n", "        for i in range(100): _ = db.get_bytes(f'image/{i}')\n", "    # Benchmark\n", "    start = time.perf_counter()\n", "    loaded_images, loaded_labels = [], []\n", "    with SynaDB(synadb_path) as db:\n", "        for i in range(NUM_SAMPLES):\n", "            img_bytes = db.get_bytes(f'image/{i}')\n", "            label = db.get_int(f'label/{i}')\n", "            img = np.frombuffer(img_bytes, dtype=np.uint8).reshape(IMAGE_HEIGHT, IMAGE_WIDTH)\n", "            loaded_images.append(img)\n", "            loaded_labels.append(label)\n", "    synadb_seq_time = time.perf_counter() - start\n", "    print(f'‚úì SynaDB: {NUM_SAMPLES:,} samples in {synadb_seq_time:.2f}s')\n", "    print(f'  Throughput: {NUM_SAMPLES / synadb_seq_time:,.0f} samples/sec')\n", "else:\n", "    print('‚ö†Ô∏è SynaDB not available, skipping...')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 15: HDF5 Sequential Load Benchmark\n", "hdf5_seq_time = None\n", "\n", "if HAS_H5PY:\n", "    import h5py\n", "    print('Benchmarking HDF5 sequential load...')\n", "    # Warm up\n", "    with h5py.File(hdf5_path, 'r') as f: _ = f['images'][:100]\n", "    # Benchmark\n", "    start = time.perf_counter()\n", "    with h5py.File(hdf5_path, 'r') as f:\n", "        loaded_images_h5 = f['images'][:]\n", "        loaded_labels_h5 = f['labels'][:]\n", "    hdf5_seq_time = time.perf_counter() - start\n", "    print(f'‚úì HDF5: {NUM_SAMPLES:,} samples in {hdf5_seq_time:.2f}s')\n", "    print(f'  Throughput: {NUM_SAMPLES / hdf5_seq_time:,.0f} samples/sec')\n", "else:\n", "    print('‚ö†Ô∏è h5py not available, skipping...')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 16: TFRecord Sequential Load Benchmark\n", "tfrecord_seq_time = None\n", "\n", "if HAS_TF:\n", "    import tensorflow as tf\n", "    print('Benchmarking TFRecord sequential load...')\n", "    def parse_tfrecord(example_proto):\n", "        feature_description = {'image': tf.io.FixedLenFeature([], tf.string), 'label': tf.io.FixedLenFeature([], tf.int64)}\n", "        parsed = tf.io.parse_single_example(example_proto, feature_description)\n", "        image = tf.io.decode_raw(parsed['image'], tf.uint8)\n", "        image = tf.reshape(image, [IMAGE_HEIGHT, IMAGE_WIDTH])\n", "        return image, parsed['label']\n", "    # Warm up\n", "    dataset = tf.data.TFRecordDataset(tfrecord_path)\n", "    for i, _ in enumerate(dataset.take(100)): pass\n", "    # Benchmark\n", "    start = time.perf_counter()\n", "    dataset = tf.data.TFRecordDataset(tfrecord_path).map(parse_tfrecord)\n", "    loaded_images_tf, loaded_labels_tf = [], []\n", "    for image, label in dataset:\n", "        loaded_images_tf.append(image.numpy())\n", "        loaded_labels_tf.append(label.numpy())\n", "    tfrecord_seq_time = time.perf_counter() - start\n", "    print(f'‚úì TFRecord: {NUM_SAMPLES:,} samples in {tfrecord_seq_time:.2f}s')\n", "    print(f'  Throughput: {NUM_SAMPLES / tfrecord_seq_time:,.0f} samples/sec')\n", "else:\n", "    print('‚ö†Ô∏è TensorFlow not available, skipping...')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 17: Sequential Load Results Visualization\n", "seq_throughput = {}\n", "if synadb_seq_time: seq_throughput['SynaDB'] = NUM_SAMPLES / synadb_seq_time\n", "if hdf5_seq_time: seq_throughput['HDF5'] = NUM_SAMPLES / hdf5_seq_time\n", "if tfrecord_seq_time: seq_throughput['TFRecord'] = NUM_SAMPLES / tfrecord_seq_time\n", "\n", "if seq_throughput:\n", "    fig = throughput_comparison(seq_throughput, title='Sequential Load Throughput', ylabel='Samples/second')\n", "    plt.show()\n", "else:\n", "    print('No sequential load results to display.')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## üéØ Benchmark: Random Access <a id=\"benchmark-random\"></a>\n", "\n", "Random access is critical for shuffled training. Let's measure single-sample retrieval latency."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 19: Random Access Benchmark Setup\n", "NUM_RANDOM_ACCESSES = 1000\n", "np.random.seed(SEED)\n", "random_indices = np.random.randint(0, NUM_SAMPLES, size=NUM_RANDOM_ACCESSES)\n", "print(f'Testing {NUM_RANDOM_ACCESSES:,} random accesses...')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 20: SynaDB Random Access Benchmark\n", "synadb_random_times = []\n", "\n", "if HAS_SYNADB:\n", "    from synadb import SynaDB\n", "    print('Benchmarking SynaDB random access...')\n", "    with SynaDB(synadb_path) as db:\n", "        # Warm up\n", "        for _ in range(10): _ = db.get_bytes(f'image/{random_indices[0]}')\n", "        # Benchmark\n", "        for idx in random_indices:\n", "            start = time.perf_counter()\n", "            img_bytes = db.get_bytes(f'image/{idx}')\n", "            label = db.get_int(f'label/{idx}')\n", "            img = np.frombuffer(img_bytes, dtype=np.uint8).reshape(IMAGE_HEIGHT, IMAGE_WIDTH)\n", "            elapsed = (time.perf_counter() - start) * 1000\n", "            synadb_random_times.append(elapsed)\n", "    print(f'‚úì SynaDB: Mean latency {np.mean(synadb_random_times):.3f}ms')\n", "    print(f'  P95 latency: {np.percentile(synadb_random_times, 95):.3f}ms')\n", "else:\n", "    print('‚ö†Ô∏è SynaDB not available, skipping...')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 21: HDF5 Random Access Benchmark\n", "hdf5_random_times = []\n", "\n", "if HAS_H5PY:\n", "    import h5py\n", "    print('Benchmarking HDF5 random access...')\n", "    with h5py.File(hdf5_path, 'r') as f:\n", "        # Warm up\n", "        for _ in range(10): _ = f['images'][random_indices[0]]\n", "        # Benchmark\n", "        for idx in random_indices:\n", "            start = time.perf_counter()\n", "            img = f['images'][idx]\n", "            label = f['labels'][idx]\n", "            elapsed = (time.perf_counter() - start) * 1000\n", "            hdf5_random_times.append(elapsed)\n", "    print(f'‚úì HDF5: Mean latency {np.mean(hdf5_random_times):.3f}ms')\n", "    print(f'  P95 latency: {np.percentile(hdf5_random_times, 95):.3f}ms')\n", "else:\n", "    print('‚ö†Ô∏è h5py not available, skipping...')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 22: TFRecord Random Access Note\n", "info_box('TFRecord is optimized for sequential access and does not support efficient random access. This is a key architectural difference - TFRecord requires reading from the beginning to reach a specific record.', 'TFRecord Limitation')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 23: Random Access Results Visualization\n", "random_latencies = {}\n", "if synadb_random_times: random_latencies['SynaDB'] = np.mean(synadb_random_times)\n", "if hdf5_random_times: random_latencies['HDF5'] = np.mean(hdf5_random_times)\n", "\n", "if random_latencies:\n", "    fig = bar_comparison(random_latencies, title='Random Access Latency', ylabel='Latency (ms)', lower_is_better=True)\n", "    plt.show()\n", "else:\n", "    print('No random access results to display.')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## üîÑ Benchmark: PyTorch DataLoader <a id=\"benchmark-dataloader\"></a>\n", "\n", "Let's measure training iteration throughput using PyTorch DataLoader with each format."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 25: PyTorch DataLoader Setup\n", "BATCH_SIZE = 64\n", "NUM_WORKERS = 0  # Single-threaded for fair comparison\n", "NUM_BATCHES = 100  # Number of batches to measure\n", "\n", "if HAS_TORCH:\n", "    import torch\n", "    from torch.utils.data import Dataset, DataLoader\n", "    print(f'PyTorch DataLoader benchmark: batch_size={BATCH_SIZE}, num_batches={NUM_BATCHES}')\n", "else:\n", "    print('‚ö†Ô∏è PyTorch not available, skipping DataLoader benchmarks...')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 26: SynaDB PyTorch Dataset\n", "synadb_dataloader_time = None\n", "\n", "if HAS_TORCH and HAS_SYNADB:\n", "    from synadb import SynaDB\n", "    \n", "    class SynaDBDataset(Dataset):\n", "        def __init__(self, db_path, num_samples, img_shape):\n", "            self.db_path = db_path\n", "            self.num_samples = num_samples\n", "            self.img_shape = img_shape\n", "            self.db = SynaDB(db_path)\n", "        def __len__(self): return self.num_samples\n", "        def __getitem__(self, idx):\n", "            img_bytes = self.db.get_bytes(f'image/{idx}')\n", "            label = self.db.get_int(f'label/{idx}')\n", "            img = np.frombuffer(img_bytes, dtype=np.uint8).reshape(self.img_shape)\n", "            return torch.from_numpy(img.copy()).float(), torch.tensor(label)\n", "        def close(self): self.db.close()\n", "    \n", "    print('Benchmarking SynaDB DataLoader...')\n", "    dataset = SynaDBDataset(synadb_path, NUM_SAMPLES, (IMAGE_HEIGHT, IMAGE_WIDTH))\n", "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n", "    \n", "    # Warm up\n", "    for i, _ in enumerate(dataloader):\n", "        if i >= 5: break\n", "    \n", "    # Benchmark\n", "    start = time.perf_counter()\n", "    for i, (imgs, lbls) in enumerate(dataloader):\n", "        if i >= NUM_BATCHES: break\n", "    synadb_dataloader_time = time.perf_counter() - start\n", "    dataset.close()\n", "    \n", "    print(f'‚úì SynaDB DataLoader: {NUM_BATCHES} batches in {synadb_dataloader_time:.2f}s')\n", "    print(f'  Throughput: {NUM_BATCHES * BATCH_SIZE / synadb_dataloader_time:,.0f} samples/sec')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 27: HDF5 PyTorch Dataset\n", "hdf5_dataloader_time = None\n", "\n", "if HAS_TORCH and HAS_H5PY:\n", "    import h5py\n", "    \n", "    class HDF5Dataset(Dataset):\n", "        def __init__(self, h5_path):\n", "            self.h5_path = h5_path\n", "            self.file = h5py.File(h5_path, 'r')\n", "            self.images = self.file['images']\n", "            self.labels = self.file['labels']\n", "        def __len__(self): return len(self.images)\n", "        def __getitem__(self, idx):\n", "            img = self.images[idx]\n", "            label = self.labels[idx]\n", "            return torch.from_numpy(img.copy()).float(), torch.tensor(label)\n", "        def close(self): self.file.close()\n", "    \n", "    print('Benchmarking HDF5 DataLoader...')\n", "    dataset = HDF5Dataset(hdf5_path)\n", "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n", "    \n", "    # Warm up\n", "    for i, _ in enumerate(dataloader):\n", "        if i >= 5: break\n", "    \n", "    # Benchmark\n", "    start = time.perf_counter()\n", "    for i, (imgs, lbls) in enumerate(dataloader):\n", "        if i >= NUM_BATCHES: break\n", "    hdf5_dataloader_time = time.perf_counter() - start\n", "    dataset.close()\n", "    \n", "    print(f'‚úì HDF5 DataLoader: {NUM_BATCHES} batches in {hdf5_dataloader_time:.2f}s')\n", "    print(f'  Throughput: {NUM_BATCHES * BATCH_SIZE / hdf5_dataloader_time:,.0f} samples/sec')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 28: DataLoader Results Visualization\n", "dataloader_throughput = {}\n", "if synadb_dataloader_time: dataloader_throughput['SynaDB'] = NUM_BATCHES * BATCH_SIZE / synadb_dataloader_time\n", "if hdf5_dataloader_time: dataloader_throughput['HDF5'] = NUM_BATCHES * BATCH_SIZE / hdf5_dataloader_time\n", "\n", "if dataloader_throughput:\n", "    fig = throughput_comparison(dataloader_throughput, title='PyTorch DataLoader Throughput', ylabel='Samples/second')\n", "    plt.show()\n", "else:\n", "    print('No DataLoader results to display.')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## üßÆ Tensor Extraction Comparison <a id=\"tensor-extraction\"></a>\n", "\n", "SynaDB provides native tensor extraction capabilities. Let's compare how each format handles extracting data as numpy arrays."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 30: Tensor Extraction Comparison\n", "print('Tensor Extraction Comparison\\n')\n", "print('=' * 60)\n", "\n", "# SynaDB tensor extraction\n", "if HAS_SYNADB:\n", "    from synadb import SynaDB\n", "    print('\\nüì¶ SynaDB Tensor Extraction:')\n", "    print('  - Native get_history_tensor() for time-series data')\n", "    print('  - Direct numpy array output')\n", "    print('  - Schema-free: mix different data types')\n", "    with SynaDB(synadb_path) as db:\n", "        # Store some float values for tensor extraction demo\n", "        for i in range(100):\n", "            db.put_float('metrics/loss', 1.0 / (i + 1))\n", "        # Extract as tensor\n", "        loss_tensor = db.get_history_tensor('metrics/loss')\n", "        print(f'  ‚úì Extracted tensor shape: {loss_tensor.shape}')\n", "\n", "# HDF5 tensor extraction\n", "if HAS_H5PY:\n", "    import h5py\n", "    print('\\nüì¶ HDF5 Tensor Extraction:')\n", "    print('  - Direct array slicing with [:] syntax')\n", "    print('  - Chunked access for large datasets')\n", "    print('  - Fixed schema required')\n", "    with h5py.File(hdf5_path, 'r') as f:\n", "        sample_tensor = f['images'][:100]\n", "        print(f'  ‚úì Extracted tensor shape: {sample_tensor.shape}')\n", "\n", "# TFRecord tensor extraction\n", "if HAS_TF:\n", "    print('\\nüì¶ TFRecord Tensor Extraction:')\n", "    print('  - Requires parsing through tf.data pipeline')\n", "    print('  - Optimized for streaming, not random access')\n", "    print('  - Best for TensorFlow training pipelines')\n", "\n", "print('\\n' + '=' * 60)"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## üîÑ Schema Flexibility Demo <a id=\"schema-flexibility\"></a>\n", "\n", "One of SynaDB's key advantages is schema flexibility - you can add new data types without migration."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 32: Schema Flexibility Demo\n", "print('Schema Flexibility Demonstration\\n')\n", "print('=' * 60)\n", "\n", "if HAS_SYNADB:\n", "    from synadb import SynaDB\n", "    print('\\nüì¶ SynaDB Schema Flexibility:')\n", "    with SynaDB(synadb_path) as db:\n", "        # Add new data types without any schema changes\n", "        db.put_text('metadata/description', 'MNIST-like synthetic dataset')\n", "        db.put_int('metadata/num_samples', NUM_SAMPLES)\n", "        db.put_float('metadata/version', 1.0)\n", "        db.put_bytes('metadata/config', b'{\"augmentation\": true}')\n", "        \n", "        print('  ‚úì Added text metadata')\n", "        print('  ‚úì Added integer metadata')\n", "        print('  ‚úì Added float metadata')\n", "        print('  ‚úì Added bytes metadata (JSON config)')\n", "        print('  ‚Üí No schema migration required!')\n", "\n", "if HAS_H5PY:\n", "    import h5py\n", "    print('\\nüì¶ HDF5 Schema Changes:')\n", "    with h5py.File(hdf5_path, 'a') as f:\n", "        # Adding new datasets requires explicit creation\n", "        if 'metadata' not in f:\n", "            meta = f.create_group('metadata')\n", "            meta.attrs['description'] = 'MNIST-like synthetic dataset'\n", "            meta.attrs['num_samples'] = NUM_SAMPLES\n", "        print('  ‚úì Added metadata group with attributes')\n", "        print('  ‚Üí Requires explicit dataset/group creation')\n", "\n", "if HAS_TF:\n", "    print('\\nüì¶ TFRecord Schema Changes:')\n", "    print('  ‚ö†Ô∏è Adding new fields requires rewriting the entire file')\n", "    print('  ‚Üí Schema changes are expensive')\n", "\n", "print('\\n' + '=' * 60)"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## üìä Results Summary <a id=\"results\"></a>\n", "\n", "Let's summarize all benchmark results."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 34: Results Summary Table\n", "from IPython.display import display, Markdown, HTML\n", "\n", "# Build summary table\n", "summary_md = '''\\n| Metric | SynaDB | HDF5 | TFRecord |\\n|--------|--------|------|----------|\\n'''\n", "\n", "# Write time\n", "synadb_wt = f'{synadb_write_time:.2f}s' if synadb_write_time else 'N/A'\n", "hdf5_wt = f'{hdf5_write_time:.2f}s' if hdf5_write_time else 'N/A'\n", "tf_wt = f'{tfrecord_write_time:.2f}s' if tfrecord_write_time else 'N/A'\n", "summary_md += f'| Write Time | {synadb_wt} | {hdf5_wt} | {tf_wt} |\\n'\n", "\n", "# Sequential load\n", "synadb_sl = f'{NUM_SAMPLES/synadb_seq_time:,.0f}/s' if synadb_seq_time else 'N/A'\n", "hdf5_sl = f'{NUM_SAMPLES/hdf5_seq_time:,.0f}/s' if hdf5_seq_time else 'N/A'\n", "tf_sl = f'{NUM_SAMPLES/tfrecord_seq_time:,.0f}/s' if tfrecord_seq_time else 'N/A'\n", "summary_md += f'| Sequential Load | {synadb_sl} | {hdf5_sl} | {tf_sl} |\\n'\n", "\n", "# Random access\n", "synadb_ra = f'{np.mean(synadb_random_times):.3f}ms' if synadb_random_times else 'N/A'\n", "hdf5_ra = f'{np.mean(hdf5_random_times):.3f}ms' if hdf5_random_times else 'N/A'\n", "summary_md += f'| Random Access | {synadb_ra} | {hdf5_ra} | Not supported |\\n'\n", "\n", "# DataLoader throughput\n", "synadb_dl = f'{NUM_BATCHES*BATCH_SIZE/synadb_dataloader_time:,.0f}/s' if synadb_dataloader_time else 'N/A'\n", "hdf5_dl = f'{NUM_BATCHES*BATCH_SIZE/hdf5_dataloader_time:,.0f}/s' if hdf5_dataloader_time else 'N/A'\n", "summary_md += f'| DataLoader | {synadb_dl} | {hdf5_dl} | TF native |\\n'\n", "\n", "# Features\n", "summary_md += '| Schema Flexibility | ‚úì Excellent | ‚óã Limited | ‚úó Fixed |\\n'\n", "summary_md += '| Single File | ‚úì Yes | ‚úì Yes | ‚úì Yes |\\n'\n", "summary_md += '| Random Access | ‚úì Yes | ‚úì Yes | ‚úó No |\\n'\n", "\n", "display(Markdown(summary_md))"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 35: File Size Comparison\n", "def get_file_size(path):\n", "    if os.path.exists(path):\n", "        return os.path.getsize(path) / (1024 * 1024)  # MB\n", "    return 0\n", "\n", "file_sizes = {}\n", "if os.path.exists(synadb_path): file_sizes['SynaDB'] = get_file_size(synadb_path)\n", "if os.path.exists(hdf5_path): file_sizes['HDF5'] = get_file_size(hdf5_path)\n", "if os.path.exists(tfrecord_path): file_sizes['TFRecord'] = get_file_size(tfrecord_path)\n", "\n", "if file_sizes:\n", "    fig = memory_comparison(file_sizes, title='Storage Size Comparison', ylabel='Size (MB)')\n", "    plt.show()\n", "    print('\\nFile sizes:')\n", "    for name, size in file_sizes.items():\n", "        print(f'  {name}: {size:.1f} MB')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## üéØ Conclusions <a id=\"conclusions\"></a>"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 37: Conclusions\n", "conclusion_box(\n", "    title='Key Takeaways',\n", "    points=[\n", "        'SynaDB offers excellent schema flexibility - add new data types without migration',\n", "        'HDF5 excels at bulk sequential loading with its optimized chunked storage',\n", "        'TFRecord is best for TensorFlow pipelines but lacks random access',\n", "        'SynaDB provides a good balance of flexibility and performance for ML workflows',\n", "        'For mixed workloads (training + experimentation), SynaDB\\'s schema-free design shines',\n", "    ],\n", "    summary='Choose based on your workflow: HDF5 for pure performance, TFRecord for TensorFlow, SynaDB for flexibility and unified data management.'\n", ")"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 38: Cleanup\n", "import shutil\n", "try:\n", "    shutil.rmtree(temp_dir)\n", "    print(f'‚úì Cleaned up temp directory: {temp_dir}')\n", "except Exception as e:\n", "    print(f'‚ö†Ô∏è Could not clean up: {e}')"]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.9.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}