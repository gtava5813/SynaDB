{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 1: Header and Setup\n", "import sys\n", "sys.path.insert(0, '..')\n", "\n", "from utils.notebook_utils import display_header, display_toc, check_dependency, conclusion_box, info_box\n", "from utils.system_info import display_system_info\n", "from utils.benchmark import Benchmark, BenchmarkResult, ComparisonTable\n", "from utils.charts import setup_style, bar_comparison, throughput_comparison, memory_comparison, COLORS\n", "\n", "display_header('Tabular ML Data Comparison', 'SynaDB vs Parquet vs Arrow')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 2: Table of Contents\n", "sections = [\n", "    ('Introduction', 'introduction'),\n", "    ('Setup', 'setup'),\n", "    ('Benchmark: Column Selection', 'benchmark-column'),\n", "    ('Benchmark: Aggregations', 'benchmark-aggregation'),\n", "    ('DuckDB Integration', 'duckdb-integration'),\n", "    ('Interoperability Demo', 'interoperability'),\n", "    ('Export/Import Demo', 'export-import'),\n", "    ('Results Summary', 'results'),\n", "    ('Conclusions', 'conclusions'),\n", "]\n", "display_toc(sections)"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## üìå Introduction <a id=\"introduction\"></a>\n", "\n", "This notebook compares **SynaDB** against **Parquet** and **Arrow** for tabular ML data:\n", "\n", "| System | Type | Key Features |\n", "|--------|------|-------------|\n", "| **SynaDB** | Embedded DB | Single-file, schema-free, native ML support |\n", "| **Parquet** | File Format | Columnar, compressed, widely adopted |\n", "| **Arrow** | In-Memory Format | Zero-copy, cross-language, high performance |\n", "\n", "### What We'll Measure\n", "\n", "- **Column selection** performance (reading specific features)\n", "- **Aggregation** operations (mean, sum, groupby)\n", "- **DuckDB integration** for SQL queries\n", "- **Interoperability** between formats\n", "- **Export/Import** workflows\n", "\n", "### Test Configuration\n", "\n", "- **Dataset**: Synthetic ML feature table (1M rows, 50 columns)\n", "- **Column types**: Numeric features, categorical, timestamps"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 4: System Info\n", "display_system_info()"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## üîß Setup <a id=\"setup\"></a>\n", "\n", "Let's set up our test environment with tabular ML data."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 6: Check Dependencies and Imports\n", "import numpy as np\n", "import pandas as pd\n", "import time\n", "import os\n", "import shutil\n", "import tempfile\n", "from pathlib import Path\n", "import matplotlib.pyplot as plt\n", "\n", "# Check for SynaDB\n", "HAS_SYNADB = check_dependency('synadb', 'pip install synadb')\n", "\n", "# Check for PyArrow (includes Parquet)\n", "HAS_PYARROW = check_dependency('pyarrow', 'pip install pyarrow')\n", "\n", "# Check for DuckDB\n", "HAS_DUCKDB = check_dependency('duckdb', 'pip install duckdb')\n", "\n", "# Apply consistent styling\n", "setup_style()"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 7: Generate Tabular ML Data\n", "NUM_ROWS = 100_000  # 100K rows for demo (use 1M for production)\n", "NUM_NUMERIC_COLS = 40\n", "NUM_CATEGORICAL_COLS = 5\n", "SEED = 42\n", "\n", "print(f'Generating tabular ML dataset with {NUM_ROWS:,} rows...')\n", "np.random.seed(SEED)\n", "\n", "# Generate numeric features\n", "data = {}\n", "for i in range(NUM_NUMERIC_COLS):\n", "    data[f'feature_{i}'] = np.random.randn(NUM_ROWS).astype(np.float32)\n", "\n", "# Generate categorical features\n", "categories = ['A', 'B', 'C', 'D', 'E']\n", "for i in range(NUM_CATEGORICAL_COLS):\n", "    data[f'category_{i}'] = np.random.choice(categories, NUM_ROWS)\n", "\n", "# Generate target and metadata\n", "data['target'] = np.random.randint(0, 2, NUM_ROWS)\n", "data['timestamp'] = pd.date_range('2024-01-01', periods=NUM_ROWS, freq='s')\n", "data['user_id'] = np.random.randint(1, 10000, NUM_ROWS)\n", "\n", "df = pd.DataFrame(data)\n", "print(f'‚úì Generated DataFrame with shape: {df.shape}')\n", "print(f'‚úì Columns: {len(df.columns)}')\n", "print(f'‚úì Memory usage: {df.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 8: Create Temp Directory\n", "temp_dir = tempfile.mkdtemp(prefix='synadb_tabular_')\n", "print(f'Using temp directory: {temp_dir}')\n", "\n", "synadb_path = os.path.join(temp_dir, 'features.db')\n", "parquet_path = os.path.join(temp_dir, 'features.parquet')\n", "arrow_path = os.path.join(temp_dir, 'features.arrow')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 9: Save Data to SynaDB\n", "synadb_write_time = None\n", "\n", "if HAS_SYNADB:\n", "    from synadb import SynaDB\n", "    print('Saving data to SynaDB...')\n", "    start = time.perf_counter()\n", "    with SynaDB(synadb_path) as db:\n", "        # Store each column as a separate key\n", "        for col in df.columns:\n", "            if df[col].dtype == 'float32' or df[col].dtype == 'float64':\n", "                db.put_bytes(f'column/{col}', df[col].values.tobytes())\n", "            elif df[col].dtype == 'int64' or df[col].dtype == 'int32':\n", "                db.put_bytes(f'column/{col}', df[col].values.tobytes())\n", "            elif df[col].dtype == 'datetime64[ns]':\n", "                db.put_bytes(f'column/{col}', df[col].values.astype('int64').tobytes())\n", "            else:\n", "                # Categorical as string\n", "                db.put_text(f'column/{col}', '|'.join(df[col].astype(str)))\n", "        # Store metadata\n", "        db.put_int('metadata/num_rows', NUM_ROWS)\n", "        db.put_text('metadata/columns', '|'.join(df.columns))\n", "    synadb_write_time = time.perf_counter() - start\n", "    print(f'‚úì SynaDB: Written in {synadb_write_time:.2f}s')\n", "else:\n", "    print('‚ö†Ô∏è SynaDB not available, skipping...')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 10: Save Data to Parquet\n", "parquet_write_time = None\n", "\n", "if HAS_PYARROW:\n", "    import pyarrow as pa\n", "    import pyarrow.parquet as pq\n", "    print('Saving data to Parquet...')\n", "    start = time.perf_counter()\n", "    table = pa.Table.from_pandas(df)\n", "    pq.write_table(table, parquet_path, compression='snappy')\n", "    parquet_write_time = time.perf_counter() - start\n", "    print(f'‚úì Parquet: Written in {parquet_write_time:.2f}s')\n", "else:\n", "    print('‚ö†Ô∏è PyArrow not available, skipping...')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 11: Save Data to Arrow IPC\n", "arrow_write_time = None\n", "\n", "if HAS_PYARROW:\n", "    import pyarrow as pa\n", "    import pyarrow.feather as feather\n", "    print('Saving data to Arrow IPC (Feather)...')\n", "    start = time.perf_counter()\n", "    table = pa.Table.from_pandas(df)\n", "    feather.write_feather(table, arrow_path, compression='lz4')\n", "    arrow_write_time = time.perf_counter() - start\n", "    print(f'‚úì Arrow IPC: Written in {arrow_write_time:.2f}s')\n", "else:\n", "    print('‚ö†Ô∏è PyArrow not available, skipping...')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 12: Write Time Comparison\n", "write_times = {}\n", "if synadb_write_time: write_times['SynaDB'] = synadb_write_time\n", "if parquet_write_time: write_times['Parquet'] = parquet_write_time\n", "if arrow_write_time: write_times['Arrow'] = arrow_write_time\n", "\n", "if write_times:\n", "    fig = bar_comparison(write_times, title='Write Time (Tabular Data)', ylabel='Time (seconds)', lower_is_better=True)\n", "    plt.show()"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## üìä Benchmark: Column Selection <a id=\"benchmark-column\"></a>\n", "\n", "Let's measure how fast each format can read specific columns - critical for feature selection."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 14: Column Selection Setup\n", "# Select 5 random numeric columns\n", "np.random.seed(SEED)\n", "selected_cols = [f'feature_{i}' for i in np.random.choice(NUM_NUMERIC_COLS, 5, replace=False)]\n", "print(f'Testing column selection for: {selected_cols}')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 15: SynaDB Column Selection\n", "synadb_col_time = None\n", "\n", "if HAS_SYNADB:\n", "    from synadb import SynaDB\n", "    print('Benchmarking SynaDB column selection...')\n", "    \n", "    # Warm up\n", "    with SynaDB(synadb_path) as db:\n", "        for _ in range(3): _ = db.get_bytes(f'column/{selected_cols[0]}')\n", "    \n", "    # Benchmark\n", "    start = time.perf_counter()\n", "    with SynaDB(synadb_path) as db:\n", "        selected_data = {}\n", "        for col in selected_cols:\n", "            col_bytes = db.get_bytes(f'column/{col}')\n", "            selected_data[col] = np.frombuffer(col_bytes, dtype=np.float32)\n", "    synadb_col_time = time.perf_counter() - start\n", "    \n", "    print(f'‚úì SynaDB: {len(selected_cols)} columns in {synadb_col_time*1000:.2f}ms')\n", "    print(f'  Total values: {sum(len(v) for v in selected_data.values()):,}')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 16: Parquet Column Selection\n", "parquet_col_time = None\n", "\n", "if HAS_PYARROW:\n", "    import pyarrow.parquet as pq\n", "    print('Benchmarking Parquet column selection...')\n", "    \n", "    # Warm up\n", "    for _ in range(3): _ = pq.read_table(parquet_path, columns=[selected_cols[0]])\n", "    \n", "    # Benchmark\n", "    start = time.perf_counter()\n", "    table = pq.read_table(parquet_path, columns=selected_cols)\n", "    selected_df = table.to_pandas()\n", "    parquet_col_time = time.perf_counter() - start\n", "    \n", "    print(f'‚úì Parquet: {len(selected_cols)} columns in {parquet_col_time*1000:.2f}ms')\n", "    print(f'  Shape: {selected_df.shape}')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 17: Arrow Column Selection\n", "arrow_col_time = None\n", "\n", "if HAS_PYARROW:\n", "    import pyarrow.feather as feather\n", "    print('Benchmarking Arrow IPC column selection...')\n", "    \n", "    # Warm up\n", "    for _ in range(3): _ = feather.read_table(arrow_path, columns=[selected_cols[0]])\n", "    \n", "    # Benchmark\n", "    start = time.perf_counter()\n", "    table = feather.read_table(arrow_path, columns=selected_cols)\n", "    selected_df = table.to_pandas()\n", "    arrow_col_time = time.perf_counter() - start\n", "    \n", "    print(f'‚úì Arrow IPC: {len(selected_cols)} columns in {arrow_col_time*1000:.2f}ms')\n", "    print(f'  Shape: {selected_df.shape}')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 18: Column Selection Results\n", "col_times = {}\n", "if synadb_col_time: col_times['SynaDB'] = synadb_col_time * 1000\n", "if parquet_col_time: col_times['Parquet'] = parquet_col_time * 1000\n", "if arrow_col_time: col_times['Arrow'] = arrow_col_time * 1000\n", "\n", "if col_times:\n", "    fig = bar_comparison(col_times, title='Column Selection Time (5 columns)', ylabel='Time (ms)', lower_is_better=True)\n", "    plt.show()"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## üìà Benchmark: Aggregations <a id=\"benchmark-aggregation\"></a>\n", "\n", "Let's compare aggregation performance - common in feature engineering."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 20: SynaDB Aggregation\n", "synadb_agg_time = None\n", "\n", "if HAS_SYNADB:\n", "    from synadb import SynaDB\n", "    print('Benchmarking SynaDB aggregation...')\n", "    \n", "    start = time.perf_counter()\n", "    with SynaDB(synadb_path) as db:\n", "        # Load numeric columns and compute aggregations\n", "        results = {}\n", "        for i in range(10):  # First 10 numeric columns\n", "            col = f'feature_{i}'\n", "            col_bytes = db.get_bytes(f'column/{col}')\n", "            arr = np.frombuffer(col_bytes, dtype=np.float32)\n", "            results[col] = {'mean': np.mean(arr), 'std': np.std(arr), 'min': np.min(arr), 'max': np.max(arr)}\n", "    synadb_agg_time = time.perf_counter() - start\n", "    \n", "    print(f'‚úì SynaDB: Aggregations on 10 columns in {synadb_agg_time*1000:.2f}ms')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 21: Parquet Aggregation\n", "parquet_agg_time = None\n", "\n", "if HAS_PYARROW:\n", "    import pyarrow.parquet as pq\n", "    print('Benchmarking Parquet aggregation...')\n", "    \n", "    cols = [f'feature_{i}' for i in range(10)]\n", "    start = time.perf_counter()\n", "    table = pq.read_table(parquet_path, columns=cols)\n", "    df_subset = table.to_pandas()\n", "    results = df_subset.agg(['mean', 'std', 'min', 'max'])\n", "    parquet_agg_time = time.perf_counter() - start\n", "    \n", "    print(f'‚úì Parquet: Aggregations on 10 columns in {parquet_agg_time*1000:.2f}ms')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 22: Arrow Aggregation\n", "arrow_agg_time = None\n", "\n", "if HAS_PYARROW:\n", "    import pyarrow.feather as feather\n", "    import pyarrow.compute as pc\n", "    print('Benchmarking Arrow aggregation...')\n", "    \n", "    cols = [f'feature_{i}' for i in range(10)]\n", "    start = time.perf_counter()\n", "    table = feather.read_table(arrow_path, columns=cols)\n", "    # Use Arrow compute for aggregations\n", "    results = {}\n", "    for col in cols:\n", "        arr = table.column(col)\n", "        results[col] = {\n", "            'mean': pc.mean(arr).as_py(),\n", "            'min': pc.min(arr).as_py(),\n", "            'max': pc.max(arr).as_py()\n", "        }\n", "    arrow_agg_time = time.perf_counter() - start\n", "    \n", "    print(f'‚úì Arrow: Aggregations on 10 columns in {arrow_agg_time*1000:.2f}ms')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 23: Aggregation Results\n", "agg_times = {}\n", "if synadb_agg_time: agg_times['SynaDB'] = synadb_agg_time * 1000\n", "if parquet_agg_time: agg_times['Parquet'] = parquet_agg_time * 1000\n", "if arrow_agg_time: agg_times['Arrow'] = arrow_agg_time * 1000\n", "\n", "if agg_times:\n", "    fig = bar_comparison(agg_times, title='Aggregation Time (10 columns)', ylabel='Time (ms)', lower_is_better=True)\n", "    plt.show()"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## ü¶Ü DuckDB Integration <a id=\"duckdb-integration\"></a>\n", "\n", "DuckDB can query Parquet files directly with SQL. Let's compare this workflow."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 25: DuckDB Setup\n", "if HAS_DUCKDB:\n", "    import duckdb\n", "    print('DuckDB Integration Demo\\n')\n", "    print('DuckDB can query Parquet files directly with SQL!')\n", "else:\n", "    print('‚ö†Ô∏è DuckDB not available, skipping integration demo...')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 26: DuckDB Query on Parquet\n", "duckdb_query_time = None\n", "\n", "if HAS_DUCKDB and HAS_PYARROW:\n", "    import duckdb\n", "    print('Benchmarking DuckDB SQL query on Parquet...')\n", "    \n", "    # Complex aggregation query\n", "    query = f'''\n", "    SELECT \n", "        category_0,\n", "        COUNT(*) as count,\n", "        AVG(feature_0) as avg_f0,\n", "        AVG(feature_1) as avg_f1,\n", "        SUM(target) as total_positive\n", "    FROM read_parquet('{parquet_path}')\n", "    GROUP BY category_0\n", "    ORDER BY count DESC\n", "    '''\n", "    \n", "    # Warm up\n", "    for _ in range(3): _ = duckdb.query(query).fetchall()\n", "    \n", "    # Benchmark\n", "    start = time.perf_counter()\n", "    result = duckdb.query(query).fetchdf()\n", "    duckdb_query_time = time.perf_counter() - start\n", "    \n", "    print(f'‚úì DuckDB: Complex aggregation in {duckdb_query_time*1000:.2f}ms')\n", "    print(f'\\nResult:')\n", "    print(result)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 27: Equivalent SynaDB + Pandas Query\n", "synadb_query_time = None\n", "\n", "if HAS_SYNADB:\n", "    from synadb import SynaDB\n", "    print('Benchmarking equivalent SynaDB + Pandas query...')\n", "    \n", "    start = time.perf_counter()\n", "    with SynaDB(synadb_path) as db:\n", "        # Load required columns\n", "        cat0_str = db.get_text('column/category_0')\n", "        cat0 = cat0_str.split('|')\n", "        f0 = np.frombuffer(db.get_bytes('column/feature_0'), dtype=np.float32)\n", "        f1 = np.frombuffer(db.get_bytes('column/feature_1'), dtype=np.float32)\n", "        target = np.frombuffer(db.get_bytes('column/target'), dtype=np.int64)\n", "        \n", "        # Create DataFrame and aggregate\n", "        query_df = pd.DataFrame({'category_0': cat0, 'feature_0': f0, 'feature_1': f1, 'target': target})\n", "        result = query_df.groupby('category_0').agg(\n", "            count=('feature_0', 'count'),\n", "            avg_f0=('feature_0', 'mean'),\n", "            avg_f1=('feature_1', 'mean'),\n", "            total_positive=('target', 'sum')\n", "        ).sort_values('count', ascending=False)\n", "    synadb_query_time = time.perf_counter() - start\n", "    \n", "    print(f'‚úì SynaDB + Pandas: Complex aggregation in {synadb_query_time*1000:.2f}ms')\n", "    print(f'\\nResult:')\n", "    print(result)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 28: Query Comparison\n", "query_times = {}\n", "if synadb_query_time: query_times['SynaDB + Pandas'] = synadb_query_time * 1000\n", "if duckdb_query_time: query_times['DuckDB + Parquet'] = duckdb_query_time * 1000\n", "\n", "if query_times:\n", "    fig = bar_comparison(query_times, title='Complex Aggregation Query', ylabel='Time (ms)', lower_is_better=True)\n", "    plt.show()"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## üîÑ Interoperability Demo <a id=\"interoperability\"></a>\n", "\n", "Let's demonstrate how data can flow between these formats."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 30: Interoperability Demo\n", "print('Interoperability Demonstration\\n')\n", "print('=' * 70)\n", "\n", "print('\\nüì¶ SynaDB Interoperability:')\n", "print('  - Export to NumPy arrays (native)')\n", "print('  - Export to Pandas DataFrame (via column loading)')\n", "print('  - Import from any format that produces bytes/arrays')\n", "print('  - Best for: ML pipelines with mixed data types')\n", "\n", "print('\\nüì¶ Parquet Interoperability:')\n", "print('  - Native Arrow integration')\n", "print('  - Direct DuckDB/Spark/Pandas support')\n", "print('  - Industry standard for data lakes')\n", "print('  - Best for: Data engineering pipelines')\n", "\n", "print('\\nüì¶ Arrow Interoperability:')\n", "print('  - Zero-copy sharing between processes')\n", "print('  - Cross-language support (Python, R, Julia, etc.)')\n", "print('  - Native Parquet read/write')\n", "print('  - Best for: High-performance data exchange')\n", "\n", "print('\\n' + '=' * 70)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 31: SynaDB to Pandas Demo\n", "if HAS_SYNADB:\n", "    from synadb import SynaDB\n", "    print('SynaDB ‚Üí Pandas DataFrame Demo\\n')\n", "    \n", "    with SynaDB(synadb_path) as db:\n", "        # Load specific columns\n", "        cols_to_load = ['feature_0', 'feature_1', 'feature_2', 'target']\n", "        data = {}\n", "        for col in cols_to_load:\n", "            col_bytes = db.get_bytes(f'column/{col}')\n", "            if col == 'target':\n", "                data[col] = np.frombuffer(col_bytes, dtype=np.int64)\n", "            else:\n", "                data[col] = np.frombuffer(col_bytes, dtype=np.float32)\n", "        \n", "        result_df = pd.DataFrame(data)\n", "        print(f'Loaded DataFrame shape: {result_df.shape}')\n", "        print(result_df.head())"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 32: Arrow Zero-Copy Demo\n", "if HAS_PYARROW:\n", "    import pyarrow as pa\n", "    import pyarrow.feather as feather\n", "    print('Arrow Zero-Copy Demo\\n')\n", "    \n", "    # Read Arrow file\n", "    table = feather.read_table(arrow_path)\n", "    \n", "    # Zero-copy to NumPy\n", "    col = table.column('feature_0')\n", "    np_array = col.to_numpy(zero_copy_only=False)  # May need copy for chunked arrays\n", "    \n", "    print(f'Arrow column type: {col.type}')\n", "    print(f'NumPy array shape: {np_array.shape}')\n", "    print(f'NumPy array dtype: {np_array.dtype}')\n", "    print(f'\\nFirst 5 values: {np_array[:5]}')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## üì§ Export/Import Demo <a id=\"export-import\"></a>\n", "\n", "Let's demonstrate exporting SynaDB data to Parquet and vice versa."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 34: SynaDB to Parquet Export\n", "export_time = None\n", "\n", "if HAS_SYNADB and HAS_PYARROW:\n", "    from synadb import SynaDB\n", "    import pyarrow as pa\n", "    import pyarrow.parquet as pq\n", "    \n", "    print('Exporting SynaDB ‚Üí Parquet...')\n", "    export_path = os.path.join(temp_dir, 'exported.parquet')\n", "    \n", "    start = time.perf_counter()\n", "    with SynaDB(synadb_path) as db:\n", "        # Load numeric columns\n", "        export_data = {}\n", "        for i in range(5):  # First 5 features\n", "            col = f'feature_{i}'\n", "            col_bytes = db.get_bytes(f'column/{col}')\n", "            export_data[col] = np.frombuffer(col_bytes, dtype=np.float32)\n", "        \n", "        # Create Arrow table and write Parquet\n", "        table = pa.Table.from_pydict(export_data)\n", "        pq.write_table(table, export_path)\n", "    export_time = time.perf_counter() - start\n", "    \n", "    export_size = os.path.getsize(export_path) / (1024 * 1024)\n", "    print(f'‚úì Exported in {export_time*1000:.2f}ms')\n", "    print(f'  Output size: {export_size:.2f} MB')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 35: Parquet to SynaDB Import\n", "import_time = None\n", "\n", "if HAS_SYNADB and HAS_PYARROW:\n", "    from synadb import SynaDB\n", "    import pyarrow.parquet as pq\n", "    \n", "    print('Importing Parquet ‚Üí SynaDB...')\n", "    import_db_path = os.path.join(temp_dir, 'imported.db')\n", "    \n", "    start = time.perf_counter()\n", "    # Read Parquet\n", "    table = pq.read_table(parquet_path, columns=['feature_0', 'feature_1', 'target'])\n", "    \n", "    # Write to SynaDB\n", "    with SynaDB(import_db_path) as db:\n", "        for col_name in table.column_names:\n", "            col = table.column(col_name)\n", "            arr = col.to_numpy()\n", "            db.put_bytes(f'column/{col_name}', arr.tobytes())\n", "    import_time = time.perf_counter() - start\n", "    \n", "    import_size = os.path.getsize(import_db_path) / (1024 * 1024)\n", "    print(f'‚úì Imported in {import_time*1000:.2f}ms')\n", "    print(f'  Output size: {import_size:.2f} MB')"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 36: Export/Import Summary\n", "print('Export/Import Summary\\n')\n", "print('=' * 50)\n", "if export_time:\n", "    print(f'SynaDB ‚Üí Parquet: {export_time*1000:.2f}ms')\n", "if import_time:\n", "    print(f'Parquet ‚Üí SynaDB: {import_time*1000:.2f}ms')\n", "print('\\n‚úì Seamless data exchange between formats!')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## üìä Results Summary <a id=\"results\"></a>\n", "\n", "Let's summarize all benchmark results."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 38: Results Summary Table\n", "from IPython.display import display, Markdown\n", "\n", "# Build summary table\n", "summary_md = '''\\n| Metric | SynaDB | Parquet | Arrow |\\n|--------|--------|---------|-------|\\n'''\n", "\n", "# Write time\n", "synadb_wt = f'{synadb_write_time:.2f}s' if synadb_write_time else 'N/A'\n", "parquet_wt = f'{parquet_write_time:.2f}s' if parquet_write_time else 'N/A'\n", "arrow_wt = f'{arrow_write_time:.2f}s' if arrow_write_time else 'N/A'\n", "summary_md += f'| Write Time | {synadb_wt} | {parquet_wt} | {arrow_wt} |\\n'\n", "\n", "# Column selection\n", "synadb_cs = f'{synadb_col_time*1000:.2f}ms' if synadb_col_time else 'N/A'\n", "parquet_cs = f'{parquet_col_time*1000:.2f}ms' if parquet_col_time else 'N/A'\n", "arrow_cs = f'{arrow_col_time*1000:.2f}ms' if arrow_col_time else 'N/A'\n", "summary_md += f'| Column Selection | {synadb_cs} | {parquet_cs} | {arrow_cs} |\\n'\n", "\n", "# Aggregation\n", "synadb_ag = f'{synadb_agg_time*1000:.2f}ms' if synadb_agg_time else 'N/A'\n", "parquet_ag = f'{parquet_agg_time*1000:.2f}ms' if parquet_agg_time else 'N/A'\n", "arrow_ag = f'{arrow_agg_time*1000:.2f}ms' if arrow_agg_time else 'N/A'\n", "summary_md += f'| Aggregation | {synadb_ag} | {parquet_ag} | {arrow_ag} |\\n'\n", "\n", "# Features\n", "summary_md += '| SQL Support | Via Pandas | ‚úì DuckDB | ‚úì DuckDB |\\n'\n", "summary_md += '| Schema-Free | ‚úì Yes | ‚úó No | ‚úó No |\\n'\n", "summary_md += '| Single File | ‚úì Yes | ‚úì Yes | ‚úì Yes |\\n'\n", "summary_md += '| Compression | ‚úì LZ4 | ‚úì Snappy/Zstd | ‚úì LZ4/Zstd |\\n'\n", "\n", "display(Markdown(summary_md))"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 39: Storage Size Comparison\n", "def get_file_size(path):\n", "    if os.path.exists(path):\n", "        return os.path.getsize(path) / (1024 * 1024)\n", "    return 0\n", "\n", "file_sizes = {}\n", "if os.path.exists(synadb_path): file_sizes['SynaDB'] = get_file_size(synadb_path)\n", "if os.path.exists(parquet_path): file_sizes['Parquet'] = get_file_size(parquet_path)\n", "if os.path.exists(arrow_path): file_sizes['Arrow'] = get_file_size(arrow_path)\n", "\n", "if file_sizes:\n", "    fig = memory_comparison(file_sizes, title='Storage Size Comparison', ylabel='Size (MB)')\n", "    plt.show()\n", "    print('\\nFile sizes:')\n", "    for name, size in file_sizes.items():\n", "        print(f'  {name}: {size:.1f} MB')"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## üéØ Conclusions <a id=\"conclusions\"></a>"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 41: Conclusions\n", "conclusion_box(\n", "    title='Key Takeaways',\n", "    points=[\n", "        'Parquet excels at columnar analytics with excellent compression',\n", "        'Arrow provides fastest in-memory operations with zero-copy',\n", "        'SynaDB offers schema flexibility - mix data types without migration',\n", "        'DuckDB + Parquet is powerful for SQL analytics on files',\n", "        'SynaDB is ideal for ML workflows with evolving schemas',\n", "        'All formats support efficient column selection',\n", "    ],\n", "    summary='Choose based on workflow: Parquet for data lakes and analytics, Arrow for high-performance exchange, SynaDB for flexible ML data management.'\n", ")"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Cell 42: Cleanup\n", "import shutil\n", "try:\n", "    shutil.rmtree(temp_dir)\n", "    print(f'‚úì Cleaned up temp directory: {temp_dir}')\n", "except Exception as e:\n", "    print(f'‚ö†Ô∏è Could not clean up: {e}')"]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.9.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}