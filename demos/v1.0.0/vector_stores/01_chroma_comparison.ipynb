{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Header and Setup\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from utils.notebook_utils import display_header, display_toc, check_dependency, conclusion_box, info_box\n",
    "from utils.system_info import display_system_info\n",
    "from utils.benchmark import Benchmark, BenchmarkResult, ComparisonTable\n",
    "from utils.charts import setup_style, bar_comparison, throughput_comparison, memory_comparison, COLORS\n",
    "\n",
    "display_header('Embedded Vector Store Comparison', 'SynaDB vs Chroma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Table of Contents\n",
    "sections = [\n",
    "    ('Introduction', 'introduction'),\n",
    "    ('Setup', 'setup'),\n",
    "    ('Benchmark: Insertion', 'benchmark-insertion'),\n",
    "    ('Benchmark: Search', 'benchmark-search'),\n",
    "    ('Benchmark: Recall@k', 'benchmark-recall'),\n",
    "    ('Demo: RAG Pipeline', 'demo-rag'),\n",
    "    ('Persistence Comparison', 'persistence'),\n",
    "    ('Results Summary', 'results'),\n",
    "    ('Conclusions', 'conclusions'),\n",
    "]\n",
    "display_toc(sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udccc Introduction <a id=\"introduction\"></a>\n",
    "\n",
    "This notebook compares **SynaDB** against **Chroma**, two popular embedded vector databases.\n",
    "\n",
    "| System | Type | Key Features |\n",
    "|--------|------|-------------|\n",
    "| **SynaDB** | Embedded | Single-file, AI-native, HNSW index, FAISS backend option |\n",
    "| **Chroma** | Embedded | Popular for LLM apps, directory-based storage |\n",
    "\n",
    "### Why These Two?\n",
    "\n",
    "Both are **embedded** vector databases requiring no server. They target:\n",
    "- Local RAG applications\n",
    "- Development and prototyping\n",
    "- Single-machine deployments\n",
    "\n",
    "### What We'll Measure\n",
    "\n",
    "- **Insertion throughput** (vectors/sec)\n",
    "- **Search latency** (ms)\n",
    "- **Recall@k** (search quality)\n",
    "- **Storage size** on disk\n",
    "\n",
    "### Test Configuration\n",
    "\n",
    "- **Dataset**: 100,000 synthetic embeddings\n",
    "- **Dimensions**: 768 (sentence transformers)\n",
    "- **Queries**: 1,000 random queries\n",
    "\n",
    "> **Note**: For billion-scale search, SynaDB supports FAISS as an optional backend.\n",
    "> See `02_faiss_backend.ipynb` for details on using `--features faiss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_system_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd27 Setup <a id=\"setup\"></a>\n",
    "\n",
    "Setting up test environment with 100K synthetic embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary directory for benchmark data\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import tempfile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "HAS_SYNADB = check_dependency('synadb', 'pip install synadb')\n",
    "HAS_CHROMA = check_dependency('chromadb', 'pip install chromadb')\n",
    "setup_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure benchmark parameters and generate test data\n",
    "NUM_VECTORS = 100_000\n",
    "DIMENSIONS = 768\n",
    "NUM_QUERIES = 1000\n",
    "SEED = 42\n",
    "\n",
    "print(f'Generating {NUM_VECTORS:,} vectors...')\n",
    "np.random.seed(SEED)\n",
    "\n",
    "vectors = np.random.randn(NUM_VECTORS, DIMENSIONS).astype(np.float32)\n",
    "vectors = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "\n",
    "queries = np.random.randn(NUM_QUERIES, DIMENSIONS).astype(np.float32)\n",
    "queries = queries / np.linalg.norm(queries, axis=1, keepdims=True)\n",
    "\n",
    "keys = [f'doc_{i}' for i in range(NUM_VECTORS)]\n",
    "print(f'\u2713 Generated {NUM_VECTORS:,} vectors ({vectors.nbytes / 1024 / 1024:.1f} MB)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = tempfile.mkdtemp(prefix='synadb_benchmark_')\n",
    "synadb_path = os.path.join(temp_dir, 'synadb.db')\n",
    "chroma_path = os.path.join(temp_dir, 'chroma_db')\n",
    "print(f'Temp directory: {temp_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u26a1 Benchmark: Insertion <a id=\"benchmark-insertion\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure benchmark parameters and generate test data\n",
    "synadb_insert_time = None\n",
    "synadb_store = None\n",
    "\n",
    "if HAS_SYNADB:\n",
    "    from synadb import VectorStore\n",
    "    print('Benchmarking SynaDB insertion...')\n",
    "    synadb_store = VectorStore(synadb_path, dimensions=DIMENSIONS, metric='cosine')\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for i, (key, vec) in enumerate(zip(keys, vectors)):\n",
    "        synadb_store.insert(key, vec)\n",
    "        if (i + 1) % 20000 == 0:\n",
    "            print(f'  Inserted {i + 1:,}...')\n",
    "    synadb_insert_time = time.perf_counter() - start\n",
    "    print(f'\u2713 SynaDB: {NUM_VECTORS:,} vectors in {synadb_insert_time:.2f}s ({NUM_VECTORS/synadb_insert_time:,.0f} vec/s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure benchmark parameters and generate test data\n",
    "chroma_insert_time = None\n",
    "chroma_collection = None\n",
    "\n",
    "if HAS_CHROMA:\n",
    "    import chromadb\n",
    "    print('Benchmarking Chroma insertion...')\n",
    "    client = chromadb.PersistentClient(path=chroma_path)\n",
    "    chroma_collection = client.create_collection('benchmark', metadata={'hnsw:space': 'cosine'})\n",
    "    \n",
    "    BATCH = 5000\n",
    "    start = time.perf_counter()\n",
    "    for i in range(0, NUM_VECTORS, BATCH):\n",
    "        end = min(i + BATCH, NUM_VECTORS)\n",
    "        chroma_collection.add(ids=keys[i:end], embeddings=vectors[i:end].tolist())\n",
    "        if end % 20000 == 0:\n",
    "            print(f'  Inserted {end:,}...')\n",
    "    chroma_insert_time = time.perf_counter() - start\n",
    "    print(f'\u2713 Chroma: {NUM_VECTORS:,} vectors in {chroma_insert_time:.2f}s ({NUM_VECTORS/chroma_insert_time:,.0f} vec/s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure benchmark parameters and generate test data\n",
    "throughput = {}\n",
    "if synadb_insert_time: throughput['SynaDB'] = NUM_VECTORS / synadb_insert_time\n",
    "if chroma_insert_time: throughput['Chroma'] = NUM_VECTORS / chroma_insert_time\n",
    "if throughput:\n",
    "    throughput_comparison(throughput, title='Insertion Throughput', ylabel='Vectors/sec')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd0d Benchmark: Search <a id=\"benchmark-search\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synadb_times, synadb_results = [], []\n",
    "if HAS_SYNADB and synadb_store:\n",
    "    print('Benchmarking SynaDB search...')\n",
    "    for _ in range(5): synadb_store.search(queries[0], k=10)  # warmup\n",
    "    for i, q in enumerate(queries):\n",
    "        start = time.perf_counter()\n",
    "        results = synadb_store.search(q, k=10)\n",
    "        synadb_times.append((time.perf_counter() - start) * 1000)\n",
    "        synadb_results.append([r.key for r in results])\n",
    "    print(f'\u2713 SynaDB: mean={np.mean(synadb_times):.2f}ms, p95={np.percentile(synadb_times, 95):.2f}ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark search performance with warmup\n",
    "chroma_times, chroma_results = [], []\n",
    "if HAS_CHROMA and chroma_collection:\n",
    "    print('Benchmarking Chroma search...')\n",
    "    for _ in range(5): chroma_collection.query(query_embeddings=[queries[0].tolist()], n_results=10)\n",
    "    for i, q in enumerate(queries):\n",
    "        start = time.perf_counter()\n",
    "        res = chroma_collection.query(query_embeddings=[q.tolist()], n_results=10)\n",
    "        chroma_times.append((time.perf_counter() - start) * 1000)\n",
    "        chroma_results.append(res['ids'][0])\n",
    "    print(f'\u2713 Chroma: mean={np.mean(chroma_times):.2f}ms, p95={np.percentile(chroma_times, 95):.2f}ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark search performance with warmup\n",
    "latencies = {}\n",
    "if synadb_times: latencies['SynaDB'] = np.mean(synadb_times)\n",
    "if chroma_times: latencies['Chroma'] = np.mean(chroma_times)\n",
    "if latencies:\n",
    "    bar_comparison(latencies, title='Search Latency (k=10)', ylabel='ms', lower_is_better=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Benchmark: Recall@k <a id=\"benchmark-recall\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ground truth for recall calculation\n",
    "print('Computing ground truth...')\n",
    "ground_truth = []\n",
    "for q in queries:\n",
    "    sims = np.dot(vectors, q)\n",
    "    top_idx = np.argsort(sims)[-10:][::-1]\n",
    "    ground_truth.append([keys[i] for i in top_idx])\n",
    "print(f'\u2713 Ground truth computed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ground truth for recall calculation\n",
    "def calc_recall(pred, gt, k=10):\n",
    "    recalls = [len(set(p[:k]) & set(g[:k])) / k for p, g in zip(pred, gt)]\n",
    "    return np.mean(recalls)\n",
    "\n",
    "recall = {}\n",
    "if synadb_results: recall['SynaDB'] = calc_recall(synadb_results, ground_truth)\n",
    "if chroma_results: recall['Chroma'] = calc_recall(chroma_results, ground_truth)\n",
    "for name, val in recall.items():\n",
    "    print(f'{name} Recall@10: {val:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83e\udd16 Demo: RAG Pipeline <a id=\"demo-rag\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate RAG retrieval pipeline\n",
    "print('RAG Demo: Top-3 retrieval\\n' + '='*50)\n",
    "q = queries[0]\n",
    "if HAS_SYNADB and synadb_store:\n",
    "    print('\\n\ud83d\udce6 SynaDB:')\n",
    "    for r in synadb_store.search(q, k=3):\n",
    "        print(f'  {r.key} (score: {r.score:.4f})')\n",
    "if HAS_CHROMA and chroma_collection:\n",
    "    print('\\n\ud83d\udce6 Chroma:')\n",
    "    res = chroma_collection.query(query_embeddings=[q.tolist()], n_results=3)\n",
    "    for id, dist in zip(res['ids'][0], res['distances'][0]):\n",
    "        print(f'  {id} (distance: {dist:.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcbe Persistence Comparison <a id=\"persistence\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate RAG retrieval pipeline\n",
    "def dir_size(path):\n",
    "    if os.path.isfile(path): return os.path.getsize(path)\n",
    "    total = 0\n",
    "    for dp, dn, fn in os.walk(path):\n",
    "        for f in fn: total += os.path.getsize(os.path.join(dp, f))\n",
    "    return total\n",
    "\n",
    "storage = {}\n",
    "if os.path.exists(synadb_path):\n",
    "    storage['SynaDB'] = dir_size(synadb_path) / 1024 / 1024\n",
    "    print(f'SynaDB: {storage[\"SynaDB\"]:.1f} MB (single file)')\n",
    "if os.path.exists(chroma_path):\n",
    "    storage['Chroma'] = dir_size(chroma_path) / 1024 / 1024\n",
    "    print(f'Chroma: {storage[\"Chroma\"]:.1f} MB (directory)')\n",
    "\n",
    "if storage:\n",
    "    memory_comparison(storage, title='Storage Size', ylabel='MB')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcc8 Results Summary <a id=\"results\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize search latency comparison\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "md = '| Metric | SynaDB | Chroma |\\n|--------|--------|--------|\\n'\n",
    "if throughput:\n",
    "    md += f'| Insert (vec/s) | {throughput.get(\"SynaDB\", \"N/A\"):,.0f} | {throughput.get(\"Chroma\", \"N/A\"):,.0f} |\\n'\n",
    "if latencies:\n",
    "    md += f'| Search (ms) | {latencies.get(\"SynaDB\", \"N/A\"):.2f} | {latencies.get(\"Chroma\", \"N/A\"):.2f} |\\n'\n",
    "if recall:\n",
    "    md += f'| Recall@10 | {recall.get(\"SynaDB\", \"N/A\"):.4f} | {recall.get(\"Chroma\", \"N/A\"):.4f} |\\n'\n",
    "if storage:\n",
    "    md += f'| Storage (MB) | {storage.get(\"SynaDB\", \"N/A\"):.1f} | {storage.get(\"Chroma\", \"N/A\"):.1f} |\\n'\n",
    "display(Markdown(md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Conclusions <a id=\"conclusions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display recall@k metrics\n",
    "conclusion_box(\n",
    "    title='Key Takeaways',\n",
    "    points=[\n",
    "        '<b>SynaDB</b> uses single-file storage vs Chroma\\'s directory structure',\n",
    "        'Both achieve high recall with HNSW indexing',\n",
    "        'SynaDB includes experiment tracking, model registry, and tensor engine',\n",
    "        'For billion-scale, SynaDB supports FAISS as an optional backend',\n",
    "    ],\n",
    "    summary='Choose SynaDB for unified AI data layer with zero config. '\n",
    "            'Choose Chroma for quick LangChain prototyping.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary directory for benchmark data\n",
    "import shutil\n",
    "try:\n",
    "    shutil.rmtree(temp_dir)\n",
    "    print(f'\u2713 Cleaned up {temp_dir}')\n",
    "except: pass\n",
    "print('\\n\ud83c\udf89 Benchmark complete!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}