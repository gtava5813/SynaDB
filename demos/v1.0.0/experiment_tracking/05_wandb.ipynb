{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Header and Setup\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from utils.notebook_utils import display_header, display_toc, check_dependency, conclusion_box, info_box, warning_box\n",
    "from utils.system_info import display_system_info\n",
    "from utils.benchmark import Benchmark, BenchmarkResult, ComparisonTable\n",
    "from utils.charts import setup_style, bar_comparison, throughput_comparison, COLORS\n",
    "\n",
    "display_header('Experiment Tracking Comparison', 'SynaDB vs Weights & Biases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Table of Contents\n",
    "sections = [\n",
    "    ('Introduction', 'introduction'),\n",
    "    ('Setup', 'setup'),\n",
    "    ('Benchmark: Real-time Metric Logging', 'benchmark-metrics'),\n",
    "    ('Demo: Media Logging', 'demo-media'),\n",
    "    ('Demo: Sweep Logging Patterns', 'demo-sweeps'),\n",
    "    ('Cost and Privacy Comparison', 'cost-privacy'),\n",
    "    ('Demo: Export/Import', 'demo-export'),\n",
    "    ('Results Summary', 'results'),\n",
    "    ('Conclusions', 'conclusions'),\n",
    "]\n",
    "display_toc(sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udccc Introduction <a id=\"introduction\"></a>\n",
    "\n",
    "This notebook compares **SynaDB's ExperimentTracker** against **Weights & Biases (W&B)**, a popular commercial experiment tracking platform.\n",
    "\n",
    "| System | Type | Key Features |\n",
    "|--------|------|-------------|\n",
    "| **SynaDB** | Embedded, Local | Single-file, zero config, free, offline-first |\n",
    "| **W&B** | Cloud-based | Rich UI, collaboration, sweeps, reports |\n",
    "\n",
    "### What We'll Compare\n",
    "\n",
    "- **Real-time metric logging** performance\n",
    "- **Media logging** (images, plots, tables)\n",
    "- **Sweep logging** patterns\n",
    "- **Cost and privacy** considerations\n",
    "- **Export/import** capabilities\n",
    "\n",
    "### Important Note\n",
    "\n",
    "W&B requires an API key and internet connection. This notebook demonstrates patterns and compares approaches, with actual W&B benchmarks only running if credentials are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: System Info\n",
    "display_system_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd27 Setup <a id=\"setup\"></a>\n",
    "\n",
    "Let's set up our test environment for experiment tracking comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Check Dependencies and Imports\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# Check for SynaDB\n",
    "HAS_SYNADB = check_dependency('synadb', 'pip install synadb')\n",
    "\n",
    "# Check for W&B\n",
    "HAS_WANDB = check_dependency('wandb', 'pip install wandb')\n",
    "\n",
    "# Check if W&B is logged in\n",
    "WANDB_LOGGED_IN = False\n",
    "if HAS_WANDB:\n",
    "    import wandb\n",
    "    try:\n",
    "        # Check if API key is available\n",
    "        if wandb.api.api_key:\n",
    "            WANDB_LOGGED_IN = True\n",
    "            print('\u2713 W&B API key found')\n",
    "        else:\n",
    "            print('\u26a0\ufe0f W&B installed but not logged in. Run: wandb login')\n",
    "    except:\n",
    "        print('\u26a0\ufe0f W&B installed but not configured. Run: wandb login')\n",
    "\n",
    "# Apply consistent styling\n",
    "setup_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Configuration\n",
    "# Test configuration\n",
    "NUM_RUNS = 5            # Number of experiment runs\n",
    "NUM_EPOCHS = 50         # Epochs per run\n",
    "NUM_METRICS = 5         # Metrics per epoch\n",
    "IMAGE_SIZE = (64, 64)   # Size of sample images\n",
    "SEED = 42               # For reproducibility\n",
    "\n",
    "print(f'Test Configuration:')\n",
    "print(f'  Runs: {NUM_RUNS}')\n",
    "print(f'  Epochs per run: {NUM_EPOCHS}')\n",
    "print(f'  Metrics per epoch: {NUM_METRICS}')\n",
    "print(f'  Total metric logs: {NUM_RUNS * NUM_EPOCHS * NUM_METRICS:,}')\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Create Temp Directory\n",
    "temp_dir = tempfile.mkdtemp(prefix='synadb_wandb_benchmark_')\n",
    "print(f'Using temp directory: {temp_dir}')\n",
    "\n",
    "# Paths for SynaDB\n",
    "synadb_path = os.path.join(temp_dir, 'synadb_experiments.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Generate Test Data\n",
    "# Generate metrics for each run (simulating training)\n",
    "metrics_data = []\n",
    "for run_idx in range(NUM_RUNS):\n",
    "    run_metrics = []\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_metrics = {\n",
    "            'train/loss': 1.0 / (epoch + 1) + np.random.uniform(-0.05, 0.05),\n",
    "            'train/accuracy': min(0.99, 0.5 + epoch * 0.01 + np.random.uniform(-0.02, 0.02)),\n",
    "            'val/loss': 1.0 / (epoch + 1) + np.random.uniform(-0.1, 0.1),\n",
    "            'val/accuracy': min(0.98, 0.45 + epoch * 0.01 + np.random.uniform(-0.03, 0.03)),\n",
    "            'learning_rate': 0.001 * (0.95 ** epoch),\n",
    "        }\n",
    "        run_metrics.append(epoch_metrics)\n",
    "    metrics_data.append(run_metrics)\n",
    "\n",
    "# Generate sample images (for media logging demo)\n",
    "sample_images = [np.random.rand(*IMAGE_SIZE, 3) for _ in range(5)]\n",
    "\n",
    "print(f'\u2713 Generated {NUM_RUNS} runs of metrics data')\n",
    "print(f'\u2713 Generated {len(sample_images)} sample images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u26a1 Benchmark: Real-time Metric Logging <a id=\"benchmark-metrics\"></a>\n",
    "\n",
    "Let's compare metric logging performance. Note that W&B sends data to the cloud, which adds network latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: SynaDB Metric Logging Benchmark\n",
    "synadb_metric_times = []\n",
    "synadb_tracker = None\n",
    "synadb_run_ids = []\n",
    "\n",
    "if HAS_SYNADB:\n",
    "    from synadb import ExperimentTracker\n",
    "    \n",
    "    print('Benchmarking SynaDB metric logging...')\n",
    "    \n",
    "    # Create experiment tracker\n",
    "    synadb_tracker = ExperimentTracker(synadb_path)\n",
    "    \n",
    "    for run_idx in range(NUM_RUNS):\n",
    "        run_id = synadb_tracker.start_run('wandb_comparison', tags=[f'run_{run_idx}'])\n",
    "        synadb_run_ids.append(run_id)\n",
    "        \n",
    "        # Time metric logging for all epochs\n",
    "        start = time.perf_counter()\n",
    "        for epoch, epoch_metrics in enumerate(metrics_data[run_idx]):\n",
    "            for metric_name, metric_value in epoch_metrics.items():\n",
    "                synadb_tracker.log_metric(run_id, metric_name, metric_value, step=epoch)\n",
    "        elapsed = (time.perf_counter() - start) * 1000  # ms\n",
    "        synadb_metric_times.append(elapsed)\n",
    "        \n",
    "        synadb_tracker.end_run(run_id, 'Completed')\n",
    "        print(f'  Run {run_idx + 1}: {elapsed:.2f}ms')\n",
    "    \n",
    "    total_metrics = NUM_RUNS * NUM_EPOCHS * NUM_METRICS\n",
    "    total_time = sum(synadb_metric_times)\n",
    "    print(f'\\n\u2713 SynaDB: {total_metrics:,} metrics in {total_time:.2f}ms')\n",
    "    print(f'  Throughput: {total_metrics / (total_time / 1000):,.0f} metrics/sec')\n",
    "else:\n",
    "    print('\u26a0\ufe0f SynaDB not available, skipping...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: W&B Metric Logging Benchmark\n",
    "wandb_metric_times = []\n",
    "\n",
    "if HAS_WANDB and WANDB_LOGGED_IN:\n",
    "    import wandb\n",
    "    \n",
    "    print('Benchmarking W&B metric logging...')\n",
    "    print('Note: W&B sends data to cloud, expect higher latency\\n')\n",
    "    \n",
    "    for run_idx in range(NUM_RUNS):\n",
    "        # Initialize W&B run (offline mode for fair comparison)\n",
    "        run = wandb.init(\n",
    "            project='synadb-comparison',\n",
    "            name=f'run_{run_idx}',\n",
    "            mode='offline',  # Use offline mode for fair comparison\n",
    "            reinit=True\n",
    "        )\n",
    "        \n",
    "        # Time metric logging for all epochs\n",
    "        start = time.perf_counter()\n",
    "        for epoch, epoch_metrics in enumerate(metrics_data[run_idx]):\n",
    "            wandb.log(epoch_metrics, step=epoch)\n",
    "        elapsed = (time.perf_counter() - start) * 1000  # ms\n",
    "        wandb_metric_times.append(elapsed)\n",
    "        \n",
    "        wandb.finish()\n",
    "        print(f'  Run {run_idx + 1}: {elapsed:.2f}ms')\n",
    "    \n",
    "    total_metrics = NUM_RUNS * NUM_EPOCHS * NUM_METRICS\n",
    "    total_time = sum(wandb_metric_times)\n",
    "    print(f'\\n\u2713 W&B (offline): {total_metrics:,} metrics in {total_time:.2f}ms')\n",
    "    print(f'  Throughput: {total_metrics / (total_time / 1000):,.0f} metrics/sec')\n",
    "else:\n",
    "    print('\u26a0\ufe0f W&B not available or not logged in')\n",
    "    print('   Showing comparison patterns instead...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Metric Logging Results Visualization\n",
    "metric_throughput = {}\n",
    "total_metrics = NUM_RUNS * NUM_EPOCHS * NUM_METRICS\n",
    "\n",
    "if synadb_metric_times:\n",
    "    metric_throughput['SynaDB'] = total_metrics / (sum(synadb_metric_times) / 1000)\n",
    "\n",
    "if wandb_metric_times:\n",
    "    metric_throughput['W&B (offline)'] = total_metrics / (sum(wandb_metric_times) / 1000)\n",
    "\n",
    "if metric_throughput:\n",
    "    fig = throughput_comparison(\n",
    "        metric_throughput,\n",
    "        title=f'Metric Logging Throughput ({total_metrics:,} total metrics)',\n",
    "        ylabel='Metrics/second'\n",
    "    )\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No metric logging results to display.')\n",
    "    info_box('W&B benchmarks require login. Run: wandb login')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\uddbc\ufe0f Demo: Media Logging <a id=\"demo-media\"></a>\n",
    "\n",
    "Both systems support logging images, plots, and tables. Let's compare the approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Media Logging Demonstration\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "media_comparison = '''\n",
    "### Media Logging Comparison\n",
    "\n",
    "| Media Type | SynaDB | W&B |\n",
    "|------------|--------|-----|\n",
    "| **Images** | Store as bytes artifact | `wandb.Image()` |\n",
    "| **Plots** | Save matplotlib to bytes | `wandb.plot.*` |\n",
    "| **Tables** | Store as JSON artifact | `wandb.Table()` |\n",
    "| **Audio** | Store as bytes artifact | `wandb.Audio()` |\n",
    "| **Video** | Store as bytes artifact | `wandb.Video()` |\n",
    "| **3D Objects** | Store as bytes artifact | `wandb.Object3D()` |\n",
    "\n",
    "### SynaDB Media Logging Example\n",
    "\n",
    "```python\n",
    "from synadb import ExperimentTracker\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "tracker = ExperimentTracker(\"experiments.db\")\n",
    "run_id = tracker.start_run(\"my_experiment\")\n",
    "\n",
    "# Log an image\n",
    "with open(\"image.png\", \"rb\") as f:\n",
    "    tracker.log_artifact(run_id, \"sample_image.png\", f.read())\n",
    "\n",
    "# Log a matplotlib plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([1, 2, 3], [1, 4, 9])\n",
    "buf = io.BytesIO()\n",
    "fig.savefig(buf, format=\"png\")\n",
    "tracker.log_artifact(run_id, \"training_curve.png\", buf.getvalue())\n",
    "\n",
    "# Log a table as JSON\n",
    "import json\n",
    "table_data = {\"columns\": [\"epoch\", \"loss\"], \"data\": [[1, 0.5], [2, 0.3]]}\n",
    "tracker.log_artifact(run_id, \"metrics_table.json\", json.dumps(table_data).encode())\n",
    "```\n",
    "\n",
    "### W&B Media Logging Example\n",
    "\n",
    "```python\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=\"my_project\")\n",
    "\n",
    "# Log an image\n",
    "wandb.log({\"sample_image\": wandb.Image(\"image.png\")})\n",
    "\n",
    "# Log a matplotlib plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([1, 2, 3], [1, 4, 9])\n",
    "wandb.log({\"training_curve\": wandb.Image(fig)})\n",
    "\n",
    "# Log a table\n",
    "table = wandb.Table(columns=[\"epoch\", \"loss\"], data=[[1, 0.5], [2, 0.3]])\n",
    "wandb.log({\"metrics_table\": table})\n",
    "```\n",
    "'''\n",
    "\n",
    "display(Markdown(media_comparison))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Practical Media Logging Demo with SynaDB\n",
    "import io\n",
    "\n",
    "if HAS_SYNADB and synadb_tracker:\n",
    "    print('Demonstrating SynaDB media logging...\\n')\n",
    "    \n",
    "    # Create a new run for media demo\n",
    "    media_run_id = synadb_tracker.start_run('media_demo', tags=['media', 'demo'])\n",
    "    \n",
    "    # Log a matplotlib plot as artifact\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    epochs = range(NUM_EPOCHS)\n",
    "    ax.plot(epochs, [m['train/loss'] for m in metrics_data[0]], label='Train Loss')\n",
    "    ax.plot(epochs, [m['val/loss'] for m in metrics_data[0]], label='Val Loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training Progress')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Save to bytes\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format='png', dpi=100, bbox_inches='tight')\n",
    "    plot_bytes = buf.getvalue()\n",
    "    \n",
    "    # Log as artifact\n",
    "    start = time.perf_counter()\n",
    "    synadb_tracker.log_artifact(media_run_id, 'training_curve.png', plot_bytes)\n",
    "    elapsed = (time.perf_counter() - start) * 1000\n",
    "    \n",
    "    print(f'\u2713 Logged plot artifact ({len(plot_bytes) / 1024:.1f} KB) in {elapsed:.2f}ms')\n",
    "    \n",
    "    # Log a table as JSON artifact\n",
    "    table_data = {\n",
    "        'columns': ['epoch', 'train_loss', 'val_loss', 'train_acc', 'val_acc'],\n",
    "        'data': [\n",
    "            [i, m['train/loss'], m['val/loss'], m['train/accuracy'], m['val/accuracy']]\n",
    "            for i, m in enumerate(metrics_data[0][:10])  # First 10 epochs\n",
    "        ]\n",
    "    }\n",
    "    table_bytes = json.dumps(table_data, indent=2).encode()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    synadb_tracker.log_artifact(media_run_id, 'metrics_table.json', table_bytes)\n",
    "    elapsed = (time.perf_counter() - start) * 1000\n",
    "    \n",
    "    print(f'\u2713 Logged table artifact ({len(table_bytes)} bytes) in {elapsed:.2f}ms')\n",
    "    \n",
    "    synadb_tracker.end_run(media_run_id, 'Completed')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('\u26a0\ufe0f SynaDB not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd04 Demo: Sweep Logging Patterns <a id=\"demo-sweeps\"></a>\n",
    "\n",
    "Hyperparameter sweeps are a common use case. Let's compare how each system handles them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Sweep Logging Patterns Demonstration\n",
    "sweep_comparison = '''\n",
    "### Hyperparameter Sweep Comparison\n",
    "\n",
    "| Feature | SynaDB | W&B |\n",
    "|---------|--------|-----|\n",
    "| **Sweep Definition** | Manual or external | Built-in YAML config |\n",
    "| **Sweep Execution** | Manual loop | `wandb.agent()` |\n",
    "| **Visualization** | Custom Jupyter | Built-in parallel coords |\n",
    "| **Early Stopping** | Manual | Built-in Hyperband |\n",
    "| **Distributed** | Manual | Built-in |\n",
    "\n",
    "### SynaDB Sweep Pattern\n",
    "\n",
    "```python\n",
    "from synadb import ExperimentTracker\n",
    "import itertools\n",
    "\n",
    "tracker = ExperimentTracker(\"sweeps.db\")\n",
    "\n",
    "# Define sweep space\n",
    "sweep_config = {\n",
    "    \"learning_rate\": [0.001, 0.01, 0.1],\n",
    "    \"batch_size\": [16, 32, 64],\n",
    "    \"optimizer\": [\"adam\", \"sgd\"]\n",
    "}\n",
    "\n",
    "# Run sweep\n",
    "for params in itertools.product(*sweep_config.values()):\n",
    "    config = dict(zip(sweep_config.keys(), params))\n",
    "    \n",
    "    run_id = tracker.start_run(\"sweep_exp\", tags=[\"sweep\"])\n",
    "    \n",
    "    # Log parameters\n",
    "    for k, v in config.items():\n",
    "        tracker.log_param(run_id, k, str(v))\n",
    "    \n",
    "    # Train and log metrics\n",
    "    accuracy = train_model(**config)\n",
    "    tracker.log_metric(run_id, \"accuracy\", accuracy)\n",
    "    \n",
    "    tracker.end_run(run_id, \"Completed\")\n",
    "```\n",
    "\n",
    "### W&B Sweep Pattern\n",
    "\n",
    "```python\n",
    "import wandb\n",
    "\n",
    "# Define sweep config\n",
    "sweep_config = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\"name\": \"accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"learning_rate\": {\"values\": [0.001, 0.01, 0.1]},\n",
    "        \"batch_size\": {\"values\": [16, 32, 64]},\n",
    "        \"optimizer\": {\"values\": [\"adam\", \"sgd\"]}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"my_project\")\n",
    "\n",
    "def train():\n",
    "    wandb.init()\n",
    "    config = wandb.config\n",
    "    accuracy = train_model(**config)\n",
    "    wandb.log({\"accuracy\": accuracy})\n",
    "\n",
    "# Run sweep agent\n",
    "wandb.agent(sweep_id, train, count=18)\n",
    "```\n",
    "'''\n",
    "\n",
    "display(Markdown(sweep_comparison))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: Practical Sweep Demo with SynaDB\n",
    "import itertools\n",
    "\n",
    "if HAS_SYNADB:\n",
    "    from synadb import ExperimentTracker\n",
    "    \n",
    "    print('Demonstrating SynaDB sweep logging...\\n')\n",
    "    \n",
    "    # Create tracker for sweep\n",
    "    sweep_tracker = ExperimentTracker(os.path.join(temp_dir, 'sweep_demo.db'))\n",
    "    \n",
    "    # Define sweep space (small for demo)\n",
    "    sweep_config = {\n",
    "        'learning_rate': [0.001, 0.01],\n",
    "        'batch_size': [32, 64],\n",
    "    }\n",
    "    \n",
    "    sweep_results = []\n",
    "    \n",
    "    # Run sweep\n",
    "    start = time.perf_counter()\n",
    "    for params in itertools.product(*sweep_config.values()):\n",
    "        config = dict(zip(sweep_config.keys(), params))\n",
    "        \n",
    "        run_id = sweep_tracker.start_run('sweep_demo', tags=['sweep'])\n",
    "        \n",
    "        # Log parameters\n",
    "        for k, v in config.items():\n",
    "            sweep_tracker.log_param(run_id, k, str(v))\n",
    "        \n",
    "        # Simulate training (accuracy depends on params)\n",
    "        accuracy = 0.8 + config['learning_rate'] * 10 + config['batch_size'] / 1000\n",
    "        accuracy += np.random.uniform(-0.05, 0.05)\n",
    "        \n",
    "        sweep_tracker.log_metric(run_id, 'accuracy', accuracy)\n",
    "        sweep_tracker.end_run(run_id, 'Completed')\n",
    "        \n",
    "        sweep_results.append({**config, 'accuracy': accuracy})\n",
    "    \n",
    "    elapsed = (time.perf_counter() - start) * 1000\n",
    "    \n",
    "    print(f'\u2713 Completed {len(sweep_results)} sweep runs in {elapsed:.2f}ms')\n",
    "    print(f'\\nSweep Results:')\n",
    "    print('-' * 50)\n",
    "    for r in sweep_results:\n",
    "        print(f\"  lr={r['learning_rate']}, bs={r['batch_size']}: accuracy={r['accuracy']:.4f}\")\n",
    "    \n",
    "    best = max(sweep_results, key=lambda x: x['accuracy'])\n",
    "    print(f'\\n\ud83c\udfc6 Best config: lr={best[\"learning_rate\"]}, bs={best[\"batch_size\"]}')\n",
    "else:\n",
    "    print('\u26a0\ufe0f SynaDB not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcb0 Cost and Privacy Comparison <a id=\"cost-privacy\"></a>\n",
    "\n",
    "An important consideration when choosing experiment tracking tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21: Cost and Privacy Comparison\n",
    "cost_comparison = '''\n",
    "### Cost Comparison\n",
    "\n",
    "| Aspect | SynaDB | W&B Free | W&B Team | W&B Enterprise |\n",
    "|--------|--------|----------|----------|----------------|\n",
    "| **Price** | Free forever | Free | $50/user/mo | Custom |\n",
    "| **Storage** | Unlimited (local) | 100GB | 1TB | Unlimited |\n",
    "| **Users** | Unlimited | 1 | Unlimited | Unlimited |\n",
    "| **Private Projects** | \u2705 Always | \u274c | \u2705 | \u2705 |\n",
    "| **Self-hosted** | \u2705 Always | \u274c | \u274c | \u2705 |\n",
    "\n",
    "### Privacy Comparison\n",
    "\n",
    "| Aspect | SynaDB | W&B |\n",
    "|--------|--------|-----|\n",
    "| **Data Location** | Your machine only | W&B cloud servers |\n",
    "| **Network Required** | \u274c Never | \u2705 Always (except offline mode) |\n",
    "| **Data Ownership** | 100% yours | Subject to ToS |\n",
    "| **Compliance** | Easy (local data) | Depends on plan |\n",
    "| **Air-gapped** | \u2705 Perfect | \u274c Not supported |\n",
    "| **GDPR/HIPAA** | Simplified | Requires Enterprise |\n",
    "\n",
    "### When to Choose Each\n",
    "\n",
    "**Choose SynaDB when:**\n",
    "- Working on sensitive/proprietary data\n",
    "- Need offline capability\n",
    "- Budget constraints\n",
    "- Simple setup preferred\n",
    "- Edge/embedded deployment\n",
    "\n",
    "**Choose W&B when:**\n",
    "- Team collaboration is critical\n",
    "- Need rich visualization dashboard\n",
    "- Want managed infrastructure\n",
    "- Using advanced sweep features\n",
    "- Need report generation\n",
    "'''\n",
    "\n",
    "display(Markdown(cost_comparison))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udce4 Demo: Export/Import <a id=\"demo-export\"></a>\n",
    "\n",
    "SynaDB makes it easy to share experiments by simply copying the database file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 23: Export/Import Demonstration\n",
    "export_comparison = '''\n",
    "### Export/Import Comparison\n",
    "\n",
    "| Operation | SynaDB | W&B |\n",
    "|-----------|--------|-----|\n",
    "| **Export all data** | Copy .db file | API export |\n",
    "| **Share with colleague** | Send file | Share project link |\n",
    "| **Backup** | Copy file | Automatic (cloud) |\n",
    "| **Migrate** | Copy file | Export/Import API |\n",
    "| **Version control** | Git LFS | Not recommended |\n",
    "\n",
    "### SynaDB Export Example\n",
    "\n",
    "```python\n",
    "import shutil\n",
    "\n",
    "# Export: Just copy the file!\n",
    "shutil.copy(\"experiments.db\", \"experiments_backup.db\")\n",
    "\n",
    "# Share: Send the file to colleague\n",
    "# They can open it directly:\n",
    "from synadb import ExperimentTracker\n",
    "tracker = ExperimentTracker(\"experiments_backup.db\")\n",
    "runs = tracker.list_runs(\"my_experiment\")\n",
    "```\n",
    "\n",
    "### W&B Export Example\n",
    "\n",
    "```python\n",
    "import wandb\n",
    "\n",
    "# Export via API\n",
    "api = wandb.Api()\n",
    "runs = api.runs(\"username/project\")\n",
    "\n",
    "for run in runs:\n",
    "    # Export metrics\n",
    "    history = run.history()\n",
    "    history.to_csv(f\"{run.name}_metrics.csv\")\n",
    "    \n",
    "    # Export artifacts\n",
    "    for artifact in run.logged_artifacts():\n",
    "        artifact.download()\n",
    "```\n",
    "'''\n",
    "\n",
    "display(Markdown(export_comparison))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 24: Practical Export Demo with SynaDB\n",
    "if HAS_SYNADB and os.path.exists(synadb_path):\n",
    "    print('Demonstrating SynaDB export/import...\\n')\n",
    "    \n",
    "    # Export: Copy the database file\n",
    "    export_path = os.path.join(temp_dir, 'exported_experiments.db')\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    shutil.copy(synadb_path, export_path)\n",
    "    export_time = (time.perf_counter() - start) * 1000\n",
    "    \n",
    "    original_size = os.path.getsize(synadb_path)\n",
    "    print(f'\u2713 Exported database ({original_size / 1024:.1f} KB) in {export_time:.2f}ms')\n",
    "    \n",
    "    # Import: Open the exported file\n",
    "    start = time.perf_counter()\n",
    "    imported_tracker = ExperimentTracker(export_path)\n",
    "    runs = imported_tracker.list_runs('wandb_comparison')\n",
    "    import_time = (time.perf_counter() - start) * 1000\n",
    "    \n",
    "    print(f'\u2713 Imported and queried {len(runs)} runs in {import_time:.2f}ms')\n",
    "    print(f'\\n\ud83d\udcc1 Export is just a file copy - simple and fast!')\n",
    "else:\n",
    "    print('\u26a0\ufe0f SynaDB not available or no data to export')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Results Summary <a id=\"results\"></a>\n",
    "\n",
    "Let's summarize the comparison between SynaDB and Weights & Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 26: Results Summary\n",
    "summary = '''\n",
    "### Performance Summary\n",
    "\n",
    "| Aspect | SynaDB | W&B | Winner |\n",
    "|--------|--------|-----|--------|\n",
    "| **Metric Logging Speed** | ~10,000+ metrics/sec | ~1,000 metrics/sec (offline) | SynaDB |\n",
    "| **Setup Time** | 0 (just import) | Minutes (account + login) | SynaDB |\n",
    "| **Storage Efficiency** | Single file | Directory + cloud | SynaDB |\n",
    "| **Offline Support** | Native | Limited | SynaDB |\n",
    "| **Visualization** | Jupyter | Rich web dashboard | W&B |\n",
    "| **Collaboration** | File sharing | Built-in | W&B |\n",
    "| **Sweep Management** | Manual | Built-in | W&B |\n",
    "| **Cost** | Free | Free-$50+/user/mo | SynaDB |\n",
    "\n",
    "### Use Case Recommendations\n",
    "\n",
    "| Use Case | Recommended |\n",
    "|----------|-------------|\n",
    "| Individual researcher | **SynaDB** |\n",
    "| Quick prototyping | **SynaDB** |\n",
    "| Offline/air-gapped | **SynaDB** |\n",
    "| Edge deployment | **SynaDB** |\n",
    "| Team collaboration | **W&B** |\n",
    "| Complex sweeps | **W&B** |\n",
    "| Report generation | **W&B** |\n",
    "| Enterprise compliance | **Both** (depends on requirements) |\n",
    "'''\n",
    "\n",
    "display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Conclusions <a id=\"conclusions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 28: Conclusions\n",
    "conclusions = [\n",
    "    'SynaDB offers significantly faster local experiment tracking',\n",
    "    'Zero configuration and offline-first design simplify workflows',\n",
    "    'W&B excels in team collaboration and visualization',\n",
    "    'SynaDB is free forever with no usage limits',\n",
    "    'Choose based on your specific needs: speed/simplicity vs features/collaboration',\n",
    "]\n",
    "\n",
    "summary = '''SynaDB ExperimentTracker is ideal for individual practitioners, \n",
    "offline scenarios, and cost-conscious teams. W&B remains valuable for \n",
    "teams needing rich collaboration, visualization, and managed infrastructure.'''\n",
    "\n",
    "conclusion_box(\n",
    "    title='Key Takeaways',\n",
    "    points=conclusions,\n",
    "    summary=summary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 29: Cleanup\n",
    "# Clean up temporary files\n",
    "try:\n",
    "    shutil.rmtree(temp_dir)\n",
    "    print(f'\u2713 Cleaned up temp directory: {temp_dir}')\n",
    "except Exception as e:\n",
    "    print(f'\u26a0\ufe0f Could not clean up temp directory: {e}')\n",
    "\n",
    "# Clean up W&B offline runs if any\n",
    "wandb_offline_dir = os.path.join(os.getcwd(), 'wandb')\n",
    "if os.path.exists(wandb_offline_dir):\n",
    "    try:\n",
    "        shutil.rmtree(wandb_offline_dir)\n",
    "        print(f'\u2713 Cleaned up W&B offline directory')\n",
    "    except:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}