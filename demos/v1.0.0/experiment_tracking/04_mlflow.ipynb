{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Header and Setup\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from utils.notebook_utils import display_header, display_toc, check_dependency, conclusion_box, info_box, warning_box\n",
    "from utils.system_info import display_system_info\n",
    "from utils.benchmark import Benchmark, BenchmarkResult, ComparisonTable\n",
    "from utils.charts import setup_style, bar_comparison, throughput_comparison, COLORS\n",
    "\n",
    "display_header('Experiment Tracking Comparison', 'SynaDB vs MLflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Table of Contents\n",
    "sections = [\n",
    "    ('Introduction', 'introduction'),\n",
    "    ('Setup', 'setup'),\n",
    "    ('Benchmark: Parameter Logging', 'benchmark-params'),\n",
    "    ('Benchmark: Metric Logging', 'benchmark-metrics'),\n",
    "    ('Benchmark: Artifact Storage', 'benchmark-artifacts'),\n",
    "    ('Benchmark: Query Performance', 'benchmark-query'),\n",
    "    ('Demo: Offline Usage', 'demo-offline'),\n",
    "    ('Results Summary', 'results'),\n",
    "    ('Conclusions', 'conclusions'),\n",
    "]\n",
    "display_toc(sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udccc Introduction <a id=\"introduction\"></a>\n",
    "\n",
    "This notebook compares **SynaDB's ExperimentTracker** against **MLflow**, the most popular open-source experiment tracking platform.\n",
    "\n",
    "| System | Type | Key Features |\n",
    "|--------|------|-------------|\n",
    "| **SynaDB** | Embedded | Single-file, zero config, offline-first, AI-native |\n",
    "| **MLflow** | Server-based | Industry standard, rich UI, model registry |\n",
    "\n",
    "### What We'll Measure\n",
    "\n",
    "- **Parameter logging** latency\n",
    "- **Metric logging** throughput (100 epochs)\n",
    "- **Artifact storage** performance\n",
    "- **Query performance** for retrieving runs\n",
    "- **Setup complexity** comparison\n",
    "\n",
    "### Test Configuration\n",
    "\n",
    "- **Experiments**: 10 experiment runs\n",
    "- **Parameters**: 20 hyperparameters per run\n",
    "- **Metrics**: 100 epochs \u00d7 5 metrics per run\n",
    "- **Artifacts**: Model checkpoints (1MB each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: System Info\n",
    "display_system_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd27 Setup <a id=\"setup\"></a>\n",
    "\n",
    "Let's set up our test environment for experiment tracking comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Check Dependencies and Imports\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import uuid\n",
    "\n",
    "# Check for SynaDB\n",
    "HAS_SYNADB = check_dependency('synadb', 'pip install synadb')\n",
    "\n",
    "# Check for MLflow\n",
    "HAS_MLFLOW = check_dependency('mlflow', 'pip install mlflow')\n",
    "\n",
    "# Apply consistent styling\n",
    "setup_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Configuration\n",
    "# Test configuration\n",
    "NUM_RUNS = 10           # Number of experiment runs\n",
    "NUM_PARAMS = 20         # Parameters per run\n",
    "NUM_EPOCHS = 100        # Epochs per run\n",
    "NUM_METRICS = 5         # Metrics per epoch\n",
    "ARTIFACT_SIZE_MB = 1    # Size of model checkpoint artifacts\n",
    "SEED = 42               # For reproducibility\n",
    "\n",
    "print(f'Test Configuration:')\n",
    "print(f'  Runs: {NUM_RUNS}')\n",
    "print(f'  Parameters per run: {NUM_PARAMS}')\n",
    "print(f'  Epochs per run: {NUM_EPOCHS}')\n",
    "print(f'  Metrics per epoch: {NUM_METRICS}')\n",
    "print(f'  Total metric logs: {NUM_RUNS * NUM_EPOCHS * NUM_METRICS:,}')\n",
    "print(f'  Artifact size: {ARTIFACT_SIZE_MB} MB')\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Create Temp Directory for Databases\n",
    "temp_dir = tempfile.mkdtemp(prefix='synadb_exp_benchmark_')\n",
    "print(f'Using temp directory: {temp_dir}')\n",
    "\n",
    "# Paths for each system\n",
    "synadb_path = os.path.join(temp_dir, 'synadb_experiments.db')\n",
    "mlflow_path = os.path.join(temp_dir, 'mlruns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Generate Test Data\n",
    "# Generate hyperparameters for each run\n",
    "hyperparams = [\n",
    "    {\n",
    "        f'param_{j}': np.random.choice(['adam', 'sgd', 'rmsprop']) if j == 0\n",
    "        else np.random.uniform(0.0001, 0.1) if j == 1\n",
    "        else np.random.randint(16, 256) if j == 2\n",
    "        else np.random.uniform(0, 1)\n",
    "        for j in range(NUM_PARAMS)\n",
    "    }\n",
    "    for _ in range(NUM_RUNS)\n",
    "]\n",
    "\n",
    "# Generate metrics for each run (simulating training)\n",
    "metrics_data = []\n",
    "for run_idx in range(NUM_RUNS):\n",
    "    run_metrics = []\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_metrics = {\n",
    "            'loss': 1.0 / (epoch + 1) + np.random.uniform(-0.05, 0.05),\n",
    "            'accuracy': min(0.99, 0.5 + epoch * 0.005 + np.random.uniform(-0.02, 0.02)),\n",
    "            'val_loss': 1.0 / (epoch + 1) + np.random.uniform(-0.1, 0.1),\n",
    "            'val_accuracy': min(0.98, 0.45 + epoch * 0.005 + np.random.uniform(-0.03, 0.03)),\n",
    "            'learning_rate': 0.001 * (0.95 ** epoch),\n",
    "        }\n",
    "        run_metrics.append(epoch_metrics)\n",
    "    metrics_data.append(run_metrics)\n",
    "\n",
    "# Generate artifact data (simulated model checkpoint)\n",
    "artifact_data = np.random.bytes(ARTIFACT_SIZE_MB * 1024 * 1024)\n",
    "\n",
    "print(f'\u2713 Generated {NUM_RUNS} sets of hyperparameters')\n",
    "print(f'\u2713 Generated {NUM_RUNS * NUM_EPOCHS} epochs of metrics')\n",
    "print(f'\u2713 Generated {ARTIFACT_SIZE_MB}MB artifact data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u26a1 Benchmark: Parameter Logging <a id=\"benchmark-params\"></a>\n",
    "\n",
    "Let's measure how fast each system can log hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: SynaDB Parameter Logging Benchmark\n",
    "synadb_param_times = []\n",
    "synadb_tracker = None\n",
    "synadb_run_ids = []\n",
    "\n",
    "if HAS_SYNADB:\n",
    "    from synadb import ExperimentTracker\n",
    "    \n",
    "    print('Benchmarking SynaDB parameter logging...')\n",
    "    \n",
    "    # Create experiment tracker\n",
    "    synadb_tracker = ExperimentTracker(synadb_path)\n",
    "    \n",
    "    # Benchmark parameter logging for each run\n",
    "    for run_idx in range(NUM_RUNS):\n",
    "        # Start a new run\n",
    "        run_id = synadb_tracker.start_run('benchmark_exp', tags=[f'run_{run_idx}'])\n",
    "        synadb_run_ids.append(run_id)\n",
    "        \n",
    "        # Time parameter logging\n",
    "        start = time.perf_counter()\n",
    "        for param_name, param_value in hyperparams[run_idx].items():\n",
    "            synadb_tracker.log_param(run_id, param_name, str(param_value))\n",
    "        elapsed = (time.perf_counter() - start) * 1000  # ms\n",
    "        synadb_param_times.append(elapsed)\n",
    "        \n",
    "        if (run_idx + 1) % 5 == 0:\n",
    "            print(f'  Completed {run_idx + 1} runs...')\n",
    "    \n",
    "    print(f'\u2713 SynaDB: {NUM_RUNS} runs, {NUM_PARAMS} params each')\n",
    "    print(f'  Mean time per run: {np.mean(synadb_param_times):.2f}ms')\n",
    "    print(f'  Total params logged: {NUM_RUNS * NUM_PARAMS}')\n",
    "else:\n",
    "    print('\u26a0\ufe0f SynaDB not available, skipping...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: MLflow Parameter Logging Benchmark\n",
    "mlflow_param_times = []\n",
    "mlflow_run_ids = []\n",
    "\n",
    "if HAS_MLFLOW:\n",
    "    import mlflow\n",
    "    \n",
    "    print('Benchmarking MLflow parameter logging...')\n",
    "    \n",
    "    # Set tracking URI to local directory\n",
    "    mlflow.set_tracking_uri(f'file://{mlflow_path}')\n",
    "    mlflow.set_experiment('benchmark_exp')\n",
    "    \n",
    "    # Benchmark parameter logging for each run\n",
    "    for run_idx in range(NUM_RUNS):\n",
    "        with mlflow.start_run() as run:\n",
    "            mlflow_run_ids.append(run.info.run_id)\n",
    "            \n",
    "            # Time parameter logging\n",
    "            start = time.perf_counter()\n",
    "            for param_name, param_value in hyperparams[run_idx].items():\n",
    "                mlflow.log_param(param_name, param_value)\n",
    "            elapsed = (time.perf_counter() - start) * 1000  # ms\n",
    "            mlflow_param_times.append(elapsed)\n",
    "        \n",
    "        if (run_idx + 1) % 5 == 0:\n",
    "            print(f'  Completed {run_idx + 1} runs...')\n",
    "    \n",
    "    print(f'\u2713 MLflow: {NUM_RUNS} runs, {NUM_PARAMS} params each')\n",
    "    print(f'  Mean time per run: {np.mean(mlflow_param_times):.2f}ms')\n",
    "    print(f'  Total params logged: {NUM_RUNS * NUM_PARAMS}')\n",
    "else:\n",
    "    print('\u26a0\ufe0f MLflow not available, skipping...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Parameter Logging Results Visualization\n",
    "param_latencies = {}\n",
    "\n",
    "if synadb_param_times:\n",
    "    param_latencies['SynaDB'] = np.mean(synadb_param_times)\n",
    "\n",
    "if mlflow_param_times:\n",
    "    param_latencies['MLflow'] = np.mean(mlflow_param_times)\n",
    "\n",
    "if param_latencies:\n",
    "    fig = bar_comparison(\n",
    "        param_latencies,\n",
    "        title=f'Parameter Logging Latency ({NUM_PARAMS} params/run)',\n",
    "        ylabel='Latency (ms)',\n",
    "        lower_is_better=True\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate speedup\n",
    "    if 'SynaDB' in param_latencies and 'MLflow' in param_latencies:\n",
    "        speedup = param_latencies['MLflow'] / param_latencies['SynaDB']\n",
    "        print(f'\\n\ud83d\udcca SynaDB is {speedup:.1f}x faster for parameter logging')\n",
    "else:\n",
    "    print('No parameter logging results to display.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcc8 Benchmark: Metric Logging <a id=\"benchmark-metrics\"></a>\n",
    "\n",
    "Now let's measure metric logging throughput over 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: SynaDB Metric Logging Benchmark\n",
    "synadb_metric_times = []\n",
    "\n",
    "if HAS_SYNADB and synadb_tracker and synadb_run_ids:\n",
    "    print('Benchmarking SynaDB metric logging...')\n",
    "    \n",
    "    for run_idx, run_id in enumerate(synadb_run_ids):\n",
    "        # Time metric logging for all epochs\n",
    "        start = time.perf_counter()\n",
    "        for epoch, epoch_metrics in enumerate(metrics_data[run_idx]):\n",
    "            for metric_name, metric_value in epoch_metrics.items():\n",
    "                synadb_tracker.log_metric(run_id, metric_name, metric_value, step=epoch)\n",
    "        elapsed = (time.perf_counter() - start) * 1000  # ms\n",
    "        synadb_metric_times.append(elapsed)\n",
    "        \n",
    "        # End the run\n",
    "        synadb_tracker.end_run(run_id, 'Completed')\n",
    "        \n",
    "        if (run_idx + 1) % 5 == 0:\n",
    "            print(f'  Completed {run_idx + 1} runs...')\n",
    "    \n",
    "    total_metrics = NUM_RUNS * NUM_EPOCHS * NUM_METRICS\n",
    "    total_time = sum(synadb_metric_times)\n",
    "    print(f'\u2713 SynaDB: {total_metrics:,} metrics in {total_time:.2f}ms')\n",
    "    print(f'  Throughput: {total_metrics / (total_time / 1000):,.0f} metrics/sec')\n",
    "else:\n",
    "    print('\u26a0\ufe0f SynaDB not available, skipping...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: MLflow Metric Logging Benchmark\n",
    "mlflow_metric_times = []\n",
    "\n",
    "if HAS_MLFLOW:\n",
    "    import mlflow\n",
    "    \n",
    "    print('Benchmarking MLflow metric logging...')\n",
    "    \n",
    "    # Create new runs for metric logging (MLflow runs were ended)\n",
    "    mlflow.set_experiment('benchmark_exp_metrics')\n",
    "    \n",
    "    for run_idx in range(NUM_RUNS):\n",
    "        with mlflow.start_run():\n",
    "            # Time metric logging for all epochs\n",
    "            start = time.perf_counter()\n",
    "            for epoch, epoch_metrics in enumerate(metrics_data[run_idx]):\n",
    "                for metric_name, metric_value in epoch_metrics.items():\n",
    "                    mlflow.log_metric(metric_name, metric_value, step=epoch)\n",
    "            elapsed = (time.perf_counter() - start) * 1000  # ms\n",
    "            mlflow_metric_times.append(elapsed)\n",
    "        \n",
    "        if (run_idx + 1) % 5 == 0:\n",
    "            print(f'  Completed {run_idx + 1} runs...')\n",
    "    \n",
    "    total_metrics = NUM_RUNS * NUM_EPOCHS * NUM_METRICS\n",
    "    total_time = sum(mlflow_metric_times)\n",
    "    print(f'\u2713 MLflow: {total_metrics:,} metrics in {total_time:.2f}ms')\n",
    "    print(f'  Throughput: {total_metrics / (total_time / 1000):,.0f} metrics/sec')\n",
    "else:\n",
    "    print('\u26a0\ufe0f MLflow not available, skipping...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Metric Logging Results Visualization\n",
    "metric_throughput = {}\n",
    "total_metrics = NUM_RUNS * NUM_EPOCHS * NUM_METRICS\n",
    "\n",
    "if synadb_metric_times:\n",
    "    metric_throughput['SynaDB'] = total_metrics / (sum(synadb_metric_times) / 1000)\n",
    "\n",
    "if mlflow_metric_times:\n",
    "    metric_throughput['MLflow'] = total_metrics / (sum(mlflow_metric_times) / 1000)\n",
    "\n",
    "if metric_throughput:\n",
    "    fig = throughput_comparison(\n",
    "        metric_throughput,\n",
    "        title=f'Metric Logging Throughput ({total_metrics:,} total metrics)',\n",
    "        ylabel='Metrics/second'\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate speedup\n",
    "    if 'SynaDB' in metric_throughput and 'MLflow' in metric_throughput:\n",
    "        speedup = metric_throughput['SynaDB'] / metric_throughput['MLflow']\n",
    "        print(f'\\n\ud83d\udcca SynaDB is {speedup:.1f}x faster for metric logging')\n",
    "else:\n",
    "    print('No metric logging results to display.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udce6 Benchmark: Artifact Storage <a id=\"benchmark-artifacts\"></a>\n",
    "\n",
    "Let's compare artifact (model checkpoint) storage performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: SynaDB Artifact Storage Benchmark\n",
    "synadb_artifact_times = []\n",
    "\n",
    "if HAS_SYNADB:\n",
    "    from synadb import ExperimentTracker\n",
    "    \n",
    "    print('Benchmarking SynaDB artifact storage...')\n",
    "    \n",
    "    # Create a new tracker for artifact tests\n",
    "    synadb_artifact_tracker = ExperimentTracker(os.path.join(temp_dir, 'synadb_artifacts.db'))\n",
    "    \n",
    "    for run_idx in range(NUM_RUNS):\n",
    "        run_id = synadb_artifact_tracker.start_run('artifact_exp', tags=[f'run_{run_idx}'])\n",
    "        \n",
    "        # Time artifact logging\n",
    "        start = time.perf_counter()\n",
    "        synadb_artifact_tracker.log_artifact(run_id, f'model_checkpoint_{run_idx}.bin', artifact_data)\n",
    "        elapsed = (time.perf_counter() - start) * 1000  # ms\n",
    "        synadb_artifact_times.append(elapsed)\n",
    "        \n",
    "        synadb_artifact_tracker.end_run(run_id, 'Completed')\n",
    "        \n",
    "        if (run_idx + 1) % 5 == 0:\n",
    "            print(f'  Completed {run_idx + 1} artifacts...')\n",
    "    \n",
    "    print(f'\u2713 SynaDB: {NUM_RUNS} artifacts ({ARTIFACT_SIZE_MB}MB each)')\n",
    "    print(f'  Mean time: {np.mean(synadb_artifact_times):.2f}ms')\n",
    "    print(f'  Throughput: {ARTIFACT_SIZE_MB * 1000 / np.mean(synadb_artifact_times):.1f} MB/s')\n",
    "else:\n",
    "    print('\u26a0\ufe0f SynaDB not available, skipping...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: MLflow Artifact Storage Benchmark\n",
    "mlflow_artifact_times = []\n",
    "\n",
    "if HAS_MLFLOW:\n",
    "    import mlflow\n",
    "    \n",
    "    print('Benchmarking MLflow artifact storage...')\n",
    "    \n",
    "    mlflow.set_experiment('artifact_exp')\n",
    "    \n",
    "    # Create temp file for artifact\n",
    "    artifact_file = os.path.join(temp_dir, 'temp_artifact.bin')\n",
    "    with open(artifact_file, 'wb') as f:\n",
    "        f.write(artifact_data)\n",
    "    \n",
    "    for run_idx in range(NUM_RUNS):\n",
    "        with mlflow.start_run():\n",
    "            # Time artifact logging\n",
    "            start = time.perf_counter()\n",
    "            mlflow.log_artifact(artifact_file, f'checkpoints')\n",
    "            elapsed = (time.perf_counter() - start) * 1000  # ms\n",
    "            mlflow_artifact_times.append(elapsed)\n",
    "        \n",
    "        if (run_idx + 1) % 5 == 0:\n",
    "            print(f'  Completed {run_idx + 1} artifacts...')\n",
    "    \n",
    "    print(f'\u2713 MLflow: {NUM_RUNS} artifacts ({ARTIFACT_SIZE_MB}MB each)')\n",
    "    print(f'  Mean time: {np.mean(mlflow_artifact_times):.2f}ms')\n",
    "    print(f'  Throughput: {ARTIFACT_SIZE_MB * 1000 / np.mean(mlflow_artifact_times):.1f} MB/s')\n",
    "else:\n",
    "    print('\u26a0\ufe0f MLflow not available, skipping...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21: Artifact Storage Results Visualization\n",
    "artifact_throughput = {}\n",
    "\n",
    "if synadb_artifact_times:\n",
    "    artifact_throughput['SynaDB'] = ARTIFACT_SIZE_MB * 1000 / np.mean(synadb_artifact_times)\n",
    "\n",
    "if mlflow_artifact_times:\n",
    "    artifact_throughput['MLflow'] = ARTIFACT_SIZE_MB * 1000 / np.mean(mlflow_artifact_times)\n",
    "\n",
    "if artifact_throughput:\n",
    "    fig = throughput_comparison(\n",
    "        artifact_throughput,\n",
    "        title=f'Artifact Storage Throughput ({ARTIFACT_SIZE_MB}MB artifacts)',\n",
    "        ylabel='MB/second'\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate speedup\n",
    "    if 'SynaDB' in artifact_throughput and 'MLflow' in artifact_throughput:\n",
    "        speedup = artifact_throughput['SynaDB'] / artifact_throughput['MLflow']\n",
    "        print(f'\\n\ud83d\udcca SynaDB is {speedup:.1f}x faster for artifact storage')\n",
    "else:\n",
    "    print('No artifact storage results to display.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd0d Benchmark: Query Performance <a id=\"benchmark-query\"></a>\n",
    "\n",
    "Let's compare how fast each system can retrieve experiment runs and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 23: SynaDB Query Performance Benchmark\n",
    "synadb_query_times = []\n",
    "\n",
    "if HAS_SYNADB and synadb_tracker:\n",
    "    print('Benchmarking SynaDB query performance...')\n",
    "    \n",
    "    # Warm up\n",
    "    for _ in range(3):\n",
    "        synadb_tracker.list_runs('benchmark_exp')\n",
    "    \n",
    "    # Benchmark listing runs\n",
    "    for _ in range(100):\n",
    "        start = time.perf_counter()\n",
    "        runs = synadb_tracker.list_runs('benchmark_exp')\n",
    "        elapsed = (time.perf_counter() - start) * 1000  # ms\n",
    "        synadb_query_times.append(elapsed)\n",
    "    \n",
    "    print(f'\u2713 SynaDB: 100 list_runs queries')\n",
    "    print(f'  Mean latency: {np.mean(synadb_query_times):.2f}ms')\n",
    "    print(f'  P95 latency: {np.percentile(synadb_query_times, 95):.2f}ms')\n",
    "else:\n",
    "    print('\u26a0\ufe0f SynaDB not available, skipping...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 24: MLflow Query Performance Benchmark\n",
    "mlflow_query_times = []\n",
    "\n",
    "if HAS_MLFLOW:\n",
    "    import mlflow\n",
    "    from mlflow.tracking import MlflowClient\n",
    "    \n",
    "    print('Benchmarking MLflow query performance...')\n",
    "    \n",
    "    client = MlflowClient()\n",
    "    experiment = client.get_experiment_by_name('benchmark_exp')\n",
    "    \n",
    "    if experiment:\n",
    "        # Warm up\n",
    "        for _ in range(3):\n",
    "            client.search_runs(experiment.experiment_id)\n",
    "        \n",
    "        # Benchmark listing runs\n",
    "        for _ in range(100):\n",
    "            start = time.perf_counter()\n",
    "            runs = client.search_runs(experiment.experiment_id)\n",
    "            elapsed = (time.perf_counter() - start) * 1000  # ms\n",
    "            mlflow_query_times.append(elapsed)\n",
    "        \n",
    "        print(f'\u2713 MLflow: 100 search_runs queries')\n",
    "        print(f'  Mean latency: {np.mean(mlflow_query_times):.2f}ms')\n",
    "        print(f'  P95 latency: {np.percentile(mlflow_query_times, 95):.2f}ms')\n",
    "    else:\n",
    "        print('\u26a0\ufe0f MLflow experiment not found')\n",
    "else:\n",
    "    print('\u26a0\ufe0f MLflow not available, skipping...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 25: Query Performance Results Visualization\n",
    "query_latencies = {}\n",
    "\n",
    "if synadb_query_times:\n",
    "    query_latencies['SynaDB'] = np.mean(synadb_query_times)\n",
    "\n",
    "if mlflow_query_times:\n",
    "    query_latencies['MLflow'] = np.mean(mlflow_query_times)\n",
    "\n",
    "if query_latencies:\n",
    "    fig = bar_comparison(\n",
    "        query_latencies,\n",
    "        title='Query Latency (list runs)',\n",
    "        ylabel='Latency (ms)',\n",
    "        lower_is_better=True\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate speedup\n",
    "    if 'SynaDB' in query_latencies and 'MLflow' in query_latencies:\n",
    "        speedup = query_latencies['MLflow'] / query_latencies['SynaDB']\n",
    "        print(f'\\n\ud83d\udcca SynaDB is {speedup:.1f}x faster for queries')\n",
    "else:\n",
    "    print('No query performance results to display.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd0c Demo: Offline Usage <a id=\"demo-offline\"></a>\n",
    "\n",
    "One of SynaDB's key advantages is its offline-first design. Let's demonstrate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 27: Offline Usage Demonstration\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "offline_comparison = '''\n",
    "### Offline Capability Comparison\n",
    "\n",
    "| Feature | SynaDB | MLflow |\n",
    "|---------|--------|--------|\n",
    "| **Works offline** | \u2705 Always | \u26a0\ufe0f Local mode only |\n",
    "| **No server required** | \u2705 Embedded | \u274c Needs tracking server |\n",
    "| **Single file storage** | \u2705 One .db file | \u274c Directory structure |\n",
    "| **Zero configuration** | \u2705 Just import | \u26a0\ufe0f Set tracking URI |\n",
    "| **Air-gapped environments** | \u2705 Perfect fit | \u26a0\ufe0f Limited support |\n",
    "| **Edge deployment** | \u2705 Lightweight | \u274c Heavy dependencies |\n",
    "\n",
    "### SynaDB Offline Example\n",
    "\n",
    "```python\n",
    "# SynaDB works anywhere - no network, no server, no config\n",
    "from synadb import ExperimentTracker\n",
    "\n",
    "tracker = ExperimentTracker(\"experiments.db\")  # That's it!\n",
    "run_id = tracker.start_run(\"my_experiment\")\n",
    "tracker.log_metric(run_id, \"accuracy\", 0.95)\n",
    "tracker.end_run(run_id, \"Completed\")\n",
    "```\n",
    "\n",
    "### MLflow Local Mode\n",
    "\n",
    "```python\n",
    "# MLflow requires explicit configuration for local mode\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"file:///path/to/mlruns\")  # Must set URI\n",
    "mlflow.set_experiment(\"my_experiment\")  # Must set experiment\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_metric(\"accuracy\", 0.95)\n",
    "```\n",
    "'''\n",
    "\n",
    "display(Markdown(offline_comparison))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 28: Storage Size Comparison\n",
    "def get_dir_size(path):\n",
    "    \"\"\"Get total size of a directory in bytes.\"\"\"\n",
    "    total = 0\n",
    "    if os.path.isfile(path):\n",
    "        return os.path.getsize(path)\n",
    "    if not os.path.exists(path):\n",
    "        return 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total += os.path.getsize(fp)\n",
    "    return total\n",
    "\n",
    "def count_files(path):\n",
    "    \"\"\"Count files in a directory.\"\"\"\n",
    "    if os.path.isfile(path):\n",
    "        return 1\n",
    "    if not os.path.exists(path):\n",
    "        return 0\n",
    "    count = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        count += len(filenames)\n",
    "    return count\n",
    "\n",
    "print('Storage Comparison\\n')\n",
    "print('=' * 60)\n",
    "\n",
    "storage_sizes = {}\n",
    "\n",
    "# SynaDB\n",
    "if os.path.exists(synadb_path):\n",
    "    size = get_dir_size(synadb_path)\n",
    "    files = count_files(synadb_path)\n",
    "    storage_sizes['SynaDB'] = size / (1024 * 1024)  # MB\n",
    "    print(f'SynaDB: {size / (1024 * 1024):.2f} MB ({files} file)')\n",
    "\n",
    "# MLflow\n",
    "if os.path.exists(mlflow_path):\n",
    "    size = get_dir_size(mlflow_path)\n",
    "    files = count_files(mlflow_path)\n",
    "    storage_sizes['MLflow'] = size / (1024 * 1024)  # MB\n",
    "    print(f'MLflow: {size / (1024 * 1024):.2f} MB ({files} files)')\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "print('\\nNote: SynaDB stores everything in a single file.')\n",
    "print('MLflow uses a directory structure with many files.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Results Summary <a id=\"results\"></a>\n",
    "\n",
    "Let's summarize all benchmark results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 30: Results Summary Table\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "# Build summary table\n",
    "summary_rows = []\n",
    "\n",
    "# Parameter logging\n",
    "if synadb_param_times and mlflow_param_times:\n",
    "    synadb_param = np.mean(synadb_param_times)\n",
    "    mlflow_param = np.mean(mlflow_param_times)\n",
    "    speedup = mlflow_param / synadb_param\n",
    "    summary_rows.append(f'| Parameter Logging | {synadb_param:.2f}ms | {mlflow_param:.2f}ms | **{speedup:.1f}x** |')\n",
    "\n",
    "# Metric logging\n",
    "if synadb_metric_times and mlflow_metric_times:\n",
    "    synadb_metric = sum(synadb_metric_times)\n",
    "    mlflow_metric = sum(mlflow_metric_times)\n",
    "    speedup = mlflow_metric / synadb_metric\n",
    "    summary_rows.append(f'| Metric Logging | {synadb_metric:.0f}ms | {mlflow_metric:.0f}ms | **{speedup:.1f}x** |')\n",
    "\n",
    "# Artifact storage\n",
    "if synadb_artifact_times and mlflow_artifact_times:\n",
    "    synadb_artifact = np.mean(synadb_artifact_times)\n",
    "    mlflow_artifact = np.mean(mlflow_artifact_times)\n",
    "    speedup = mlflow_artifact / synadb_artifact\n",
    "    summary_rows.append(f'| Artifact Storage | {synadb_artifact:.2f}ms | {mlflow_artifact:.2f}ms | **{speedup:.1f}x** |')\n",
    "\n",
    "# Query performance\n",
    "if synadb_query_times and mlflow_query_times:\n",
    "    synadb_query = np.mean(synadb_query_times)\n",
    "    mlflow_query = np.mean(mlflow_query_times)\n",
    "    speedup = mlflow_query / synadb_query\n",
    "    summary_rows.append(f'| Query Performance | {synadb_query:.2f}ms | {mlflow_query:.2f}ms | **{speedup:.1f}x** |')\n",
    "\n",
    "if summary_rows:\n",
    "    summary_md = '''### Performance Summary\n",
    "\n",
    "| Benchmark | SynaDB | MLflow | Speedup |\n",
    "|-----------|--------|--------|--------|\n",
    "''' + '\\n'.join(summary_rows)\n",
    "    \n",
    "    display(Markdown(summary_md))\n",
    "else:\n",
    "    print('No results to summarize.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 31: Feature Comparison Table\n",
    "feature_comparison = '''\n",
    "### Feature Comparison\n",
    "\n",
    "| Feature | SynaDB | MLflow |\n",
    "|---------|--------|--------|\n",
    "| **Type** | Embedded | Server-based |\n",
    "| **Storage** | Single file | Directory structure |\n",
    "| **Setup** | Zero config | Requires tracking URI |\n",
    "| **Offline** | \u2705 Always works | \u26a0\ufe0f Local mode only |\n",
    "| **UI** | Jupyter integration | Web dashboard |\n",
    "| **Model Registry** | \u2705 Built-in | \u2705 Built-in |\n",
    "| **Artifact Storage** | \u2705 In database | \u2705 File system |\n",
    "| **Query API** | \u2705 Simple | \u2705 Rich filtering |\n",
    "| **Collaboration** | Export/Import | Server sharing |\n",
    "| **Dependencies** | Minimal | Many |\n",
    "'''\n",
    "\n",
    "display(Markdown(feature_comparison))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Conclusions <a id=\"conclusions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 33: Conclusions\n",
    "conclusions = [\n",
    "    'SynaDB provides significantly faster experiment tracking operations',\n",
    "    'Zero configuration makes SynaDB ideal for quick prototyping',\n",
    "    'Single-file storage simplifies backup and sharing',\n",
    "    'Offline-first design perfect for edge and air-gapped environments',\n",
    "    'MLflow offers richer UI and collaboration features for teams',\n",
    "]\n",
    "\n",
    "summary = '''SynaDB ExperimentTracker excels in performance and simplicity, \n",
    "making it ideal for individual practitioners and offline scenarios. \n",
    "MLflow remains valuable for teams needing rich collaboration features and web UI.'''\n",
    "\n",
    "conclusion_box(\n",
    "    title='Key Takeaways',\n",
    "    points=conclusions,\n",
    "    summary=summary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 34: Cleanup\n",
    "# Clean up temporary files\n",
    "import shutil\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(temp_dir)\n",
    "    print(f'\u2713 Cleaned up temp directory: {temp_dir}')\n",
    "except Exception as e:\n",
    "    print(f'\u26a0\ufe0f Could not clean up temp directory: {e}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}